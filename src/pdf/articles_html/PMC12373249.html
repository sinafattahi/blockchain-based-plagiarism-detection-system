
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Simultaneous interpreting with auto-subtitling: Investigating viewer cognitive effort, stress, and comprehension - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A1F48AF1E15305A1F400202F3E91.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="Simultaneous interpreting with auto-subtitling: Investigating viewer cognitive effort, stress, and comprehension">
<meta name="citation_author" content="Yanlin Li">
<meta name="citation_author_institution" content="Department of Industrial and Systems Engineering, Hong Kong Polytechnic University, Kowloon, Hong Kong">
<meta name="citation_author" content="Jiawen Diao">
<meta name="citation_author_institution" content="Department of Chinese and Bilingual Studies, Hong Kong Polytechnic University, Kowloon, Hong Kong">
<meta name="citation_author" content="Andrew K F Cheung">
<meta name="citation_author_institution" content="Department of Chinese and Bilingual Studies, Hong Kong Polytechnic University, Kowloon, Hong Kong">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0330692">
<meta name="citation_doi" content="10.1371/journal.pone.0330692">
<meta name="citation_pmid" content="40845000">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/pdf/pone.0330692.pdf">
<meta name="description" content="Simultaneous interpreting (SI) enables real-time cross-language communication without significant delays and is vital for fast-paced environments such as multilingual conferences. Automatic subtitles, powered by artificial intelligence (AI), is an ...">
<meta name="og:title" content="Simultaneous interpreting with auto-subtitling: Investigating viewer cognitive effort, stress, and comprehension">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Simultaneous interpreting (SI) enables real-time cross-language communication without significant delays and is vital for fast-paced environments such as multilingual conferences. Automatic subtitles, powered by artificial intelligence (AI), is an ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373249">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0330692"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0330692.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373249%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373249/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373249/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0330692" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 22;20(8):e0330692. doi: <a href="https://doi.org/10.1371/journal.pone.0330692" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330692</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Simultaneous interpreting with auto-subtitling: Investigating viewer cognitive effort, stress, and comprehension</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Yanlin Li</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Yanlin Li</span></h3>
<div class="p">
<sup>1</sup>Department of Industrial and Systems Engineering, Hong Kong Polytechnic University, Kowloon, Hong Kong</div>
<div>Conceptualization, Formal analysis, Methodology, Validation, Visualization, Writing – original draft</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yanlin Li</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Diao%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Jiawen Diao</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Jiawen Diao</span></h3>
<div class="p">
<sup>2</sup>Department of Chinese and Bilingual Studies, Hong Kong Polytechnic University, Kowloon, Hong Kong</div>
<div>Data curation, Investigation, Methodology, Project administration, Resources</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Diao%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jiawen Diao</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cheung%20AKF%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Andrew K F Cheung</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Andrew K F Cheung</span></h3>
<div class="p">
<sup>2</sup>Department of Chinese and Bilingual Studies, Hong Kong Polytechnic University, Kowloon, Hong Kong</div>
<div>Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cheung%20AKF%22%5BAuthor%5D" class="usa-link"><span class="name western">Andrew K F Cheung</span></a>
</div>
</div>
<sup>2,</sup><sup>*</sup>
</div>
<div class="cg p">Editor: <span class="name western">Robyn Berghoff</span><sup>3</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Department of Industrial and Systems Engineering, Hong Kong Polytechnic University, Kowloon, Hong Kong</div>
<div id="aff002">
<sup>2</sup>Department of Chinese and Bilingual Studies, Hong Kong Polytechnic University, Kowloon, Hong Kong</div>
<div id="edit1">
<sup>3</sup>Stellenbosch University, SOUTH AFRICA</div>
<div class="author-notes p">
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>andrew.cheung@polyu.edu.hk</span></p>
</div>
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Yanlin Li</span></strong>: <span class="role">Conceptualization, Formal analysis, Methodology, Validation, Visualization, Writing – original draft</span>
</div>
<div>
<strong class="contrib"><span class="name western">Jiawen Diao</span></strong>: <span class="role">Data curation, Investigation, Methodology, Project administration, Resources</span>
</div>
<div>
<strong class="contrib"><span class="name western">Andrew K F Cheung</span></strong>: <span class="role">Supervision, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Robyn Berghoff</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Feb 2; Accepted 2025 Aug 4; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Li et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373249  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40845000/" class="usa-link">40845000</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Simultaneous interpreting (SI) enables real-time cross-language communication without significant delays and is vital for fast-paced environments such as multilingual conferences. Automatic subtitles, powered by artificial intelligence (AI), is an important mode of audiovisual translation and has been widely deployed by virtual conferencing platforms to help users overcome language barriers. While the cognitive and emotional impacts of SI have been explored in prior studies, research directly comparing SI, auto-subtitling, and their combined use remains limited. This study investigates and compares the effectiveness of three interlingual translation modes, auto-subtitling, SI, and a combined dual-modality approach, on comprehension, cognitive load, and stress levels. Mandarin Chinese-speaking participants viewed a video presentation delivered in Arabic, a language they did not understand. Participants were divided into three groups: Group A relied on automatic subtitles in Simplified Chinese characters, Group B relied on SI in Mandarin, and Group C used a combination of both methods. Analysis of electroencephalographic data and comprehension test results revealed no statistically significant differences in content comprehension across the groups. However, Group A reported the poorest viewing experience, with the highest stress levels, while Group B expended the greatest cognitive effort. Group C exhibited the lowest levels of cognitive effort and stress, underscoring the advantages of dual-modality systems. These findings suggest that combining accurate automatic subtitles with professional interpreting may enhance accessibility, reduce cognitive demands, and improve the viewing experience, offering valuable insights into the integration of AI-driven technologies in SI.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>The purpose of this research is to experimentally examine the effects of three interlingual modes of translation, i.e., auto-subtitling, simultaneous interpreting (SI), and a combination of both (dual-modality), on viewer comprehension, cognitive load, and viewing experience during multilingual presentations. Specifically, the study evaluates whether automatic subtitles, generated by artificial intelligence (AI), can facilitate content comprehension comparably to SI, analyzes the cognitive demands experienced by viewers across these three conditions, and identifies the most effective mode for cross-linguistic communication. By systematically addressing these aspects, this research seeks to enhance audience understanding and engagement in multilingual settings.</p>
<p>The rise of multimedia communication has driven the development of audiovisual translation (AVT), encompassing subtitling, dubbing, and voiceover. Among these, subtitling is one of the most widely adopted due to its cost-effectiveness and efficiency [<a href="#pone.0330692.ref001" class="usa-link" aria-describedby="pone.0330692.ref001">1</a>,<a href="#pone.0330692.ref002" class="usa-link" aria-describedby="pone.0330692.ref002">2</a>]. Originally developed to facilitate access for deaf/hard-of-hearing audiences and those with limited proficiency in the source language, subtitling now serves critical functions in both educational and entertainment contexts [<a href="#pone.0330692.ref003" class="usa-link" aria-describedby="pone.0330692.ref003">3</a>–<a href="#pone.0330692.ref005" class="usa-link" aria-describedby="pone.0330692.ref005">5</a>]. In language education, subtitles have been shown to support vocabulary acquisition, listening comprehension, and learner engagement by providing concurrent visual and auditory input [<a href="#pone.0330692.ref006" class="usa-link" aria-describedby="pone.0330692.ref006">6</a>]. Research further highlights their role in long-term language development, particularly through autonomous exposure to captioned media (e.g., films, TV programmes) in target languages [<a href="#pone.0330692.ref007" class="usa-link" aria-describedby="pone.0330692.ref007">7</a>]. In the entertainment industry, subtitling enables cross-cultural access to audiovisual narratives, allowing global audiences to engage with original-language performances, a function amplified by streaming platforms like Netflix, YouTube, and TikTok [<a href="#pone.0330692.ref008" class="usa-link" aria-describedby="pone.0330692.ref008">8</a>].</p>
<p>Traditionally, real-time subtitling relied on stenotyping or skilled subtitlers using dual keyboards [<a href="#pone.0330692.ref001" class="usa-link" aria-describedby="pone.0330692.ref001">1</a>]. These labor-intensive methods limited scalability until the advent of AI-driven solutions. Automatic speech recognition (ASR) has transformed subtitling practices. ASR, which converts spoken language into written text using algorithms, has enabled the adoption of real-time automatic subtitles, also known as live captioning [<a href="#pone.0330692.ref009" class="usa-link" aria-describedby="pone.0330692.ref009">9</a>]. These systems are embedded in platforms like Zoom and Microsoft Teams, providing near-instantaneous subtitles to enhance accessibility for global audiences [<a href="#pone.0330692.ref008" class="usa-link" aria-describedby="pone.0330692.ref008">8</a>]. While ASR technologies now enable real-time automatic subtitles in general contexts, challenges persist in specialized, high-stakes settings, such as multilingual conferences, where SI is traditionally employed.</p>
<p>SI is a cornerstone of multilingual communication, enabling real-time exchange across linguistic boundaries in international and institutional contexts [<a href="#pone.0330692.ref001" class="usa-link" aria-describedby="pone.0330692.ref001">1</a>,<a href="#pone.0330692.ref002" class="usa-link" aria-describedby="pone.0330692.ref002">2</a>]. This mode of interpretation relies on highly skilled human interpreters who deliver accurate and immediate renditions of the source language into the target language [<a href="#pone.0330692.ref009" class="usa-link" aria-describedby="pone.0330692.ref009">9</a>]. While effective, professional SI services are resource-intensive, requiring significant human and financial investment [<a href="#pone.0330692.ref010" class="usa-link" aria-describedby="pone.0330692.ref010">10</a>]. The pandemic-driven shift to virtual and hybrid conferencing has amplified these challenges, exposing the limitations of traditional SI services in terms of scalability and affordability [<a href="#pone.0330692.ref011" class="usa-link" aria-describedby="pone.0330692.ref011">11</a>,<a href="#pone.0330692.ref012" class="usa-link" aria-describedby="pone.0330692.ref012">12</a>]. Although remote SI has emerged as a solution, it remains constrained by interpreter availability and cost. Recent advancements in auto-subtitling offer a complementary alternative. These systems provide real-time textual support to enhance accessibility in formal, multilingual conference settings, such as academic symposia and international policy forums [<a href="#pone.0330692.ref010" class="usa-link" aria-describedby="pone.0330692.ref010">10</a>]. AI-powered tools like Otter.ai and Google Meet’s live captioning are increasingly utilized to support multilingual communication in these contexts.</p>
<p>Existing research has extensively examined the advantages and limitations of subtitles across various contexts. Automatic subtitles, created using ASR technology, have been shown to enhance comprehension by bridging auditory and textual input, improving immersion, and maintaining viewer focus [<a href="#pone.0330692.ref013" class="usa-link" aria-describedby="pone.0330692.ref013">13</a>–<a href="#pone.0330692.ref017" class="usa-link" aria-describedby="pone.0330692.ref017">17</a>]. However, such advantages are not universal. When subtitle speed is high, content is dense, or synchronization is poor, the sequential processing of visual (textual) and auditory information may actually increase cognitive effort, potentially undermining comprehension [<a href="#pone.0330692.ref018" class="usa-link" aria-describedby="pone.0330692.ref018">18</a>,<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>].</p>
<p>This study focuses specifically on open (burned-in) interlingual automatic subtitles, machine-generated translations embedded directly into video, used in real-time or pre-recorded multilingual communication. A proposed enhancement in such settings is the dual-modality approach, which combines SI with auto-subtitling to distribute information across both auditory and visual channels [<a href="#pone.0330692.ref016" class="usa-link" aria-describedby="pone.0330692.ref016">16</a>]. While this multimodal design holds promise for supporting diverse processing preferences and compensating for gaps in either channel, it may also impose greater cognitive demands, as viewers are required to shift attention between listening and reading, particularly in high-stakes or fast-paced scenarios [<a href="#pone.0330692.ref011" class="usa-link" aria-describedby="pone.0330692.ref011">11</a>,<a href="#pone.0330692.ref020" class="usa-link" aria-describedby="pone.0330692.ref020">20</a>]. While auto-subtitling systems have improved, errors in transcription or translation persist, particularly in noisy audio or linguistically complex content [<a href="#pone.0330692.ref020" class="usa-link" aria-describedby="pone.0330692.ref020">20</a>].</p>
<p>While studies have explored the integration of ASR tools in interpreting workflows, they primarily focus on the impact on interpreters’ performance rather than the viewers’ experience with dual-modality systems [<a href="#pone.0330692.ref009" class="usa-link" aria-describedby="pone.0330692.ref009">9</a>]. For instance, Defrancq and Fantinuoli [<a href="#pone.0330692.ref021" class="usa-link" aria-describedby="pone.0330692.ref021">21</a>] highlight the psychological benefits of ASR availability for interpreters, while Tammasrisawat and Rangponsumrit [<a href="#pone.0330692.ref022" class="usa-link" aria-describedby="pone.0330692.ref022">22</a>] demonstrate that ASR-computer-assisted interpreting (CAI) tools can reduce error rates and omissions, improving interpreters’ terminology rendition quality. However, little is known about how the combination of automatic subtitles and SI affects viewers’ comprehension, cognitive load, and overall experience. It remains unclear whether this approach enhances understanding or introduces new challenges due to the cognitive demands of processing dual modalities.</p>
<p>This gap in the literature underscores the need for a rigorous investigation into the combined effect of auto-subtitling and SI on viewers. Such research can clarify whether the dual-modality approach strikes the right balance between accessibility and cognitive demands, offering practical insights for improving multilingual communication. To address this gap, we propose the following research questions (RQs):</p>
<ul class="list" style="list-style-type:none">
<li><p><strong>RQ1:</strong> How do automatic subtitles compare to SI in facilitating content comprehension for viewers?</p></li>
<li><p><strong>RQ2:</strong> What differences in cognitive demands do automatic subtitles and SI impose on viewers?</p></li>
<li><p><strong>RQ3:</strong> How do automatic subtitles and SI influence viewers’ overall viewing experience?</p></li>
</ul>
<p>The contributions of this study are twofold. First, it examines how audiences cognitively respond to dual modality input, which is the combination of SI and automatic subtitles, by evaluating comprehension, cognitive effort, and viewing experience in multilingual conference settings where real-time processing is essential. While prior research has focused on interpreters’ performance or single-modality delivery, this study addresses a notable gap by providing empirical analysis of audience experience in dual-modality environments, offering insights in AVT. Second, the study enhances methodological rigor by employing electroencephalographic (EEG) techniques to objectively measure viewers’ cognitive load. This approach moves beyond self-reported data to reveal how cognitive resources are allocated during dual-modality processing.</p></section><section id="sec002"><h2 class="pmc_sec_title">Literature review</h2>
<section id="sec003"><h3 class="pmc_sec_title">Automatic subtitles and the viewing experience</h3>
<p>Traditional subtitles serve as textual representations of spoken information in videos, fulfilling a variety of essential functions. Initially created to provide translations by human translators for movies and television audiences facing language barriers, subtitles have evolved into indispensable tools for language learning, cognitive load management, and improving viewer comprehension [<a href="#pone.0330692.ref018" class="usa-link" aria-describedby="pone.0330692.ref018">18</a>,<a href="#pone.0330692.ref023" class="usa-link" aria-describedby="pone.0330692.ref023">23</a>–<a href="#pone.0330692.ref025" class="usa-link" aria-describedby="pone.0330692.ref025">25</a>].</p>
<p>Automatic subtitles are generated by ASR technology that enables the transcription of spoken language into text by analyzing the shape of speech waves [<a href="#pone.0330692.ref026" class="usa-link" aria-describedby="pone.0330692.ref026">26</a>]. Unlike traditional subtitles, automatic subtitles are generated in real time, adapting to the speaker’s pace and often employing innovative formats such as multi-line stacking to improve readability [<a href="#pone.0330692.ref027" class="usa-link" aria-describedby="pone.0330692.ref027">27</a>]. Auto-subtitling has gained traction for its efficiency and scalability, particularly in environments requiring immediate content delivery, such as live broadcasts, online learning platforms, and virtual meetings [<a href="#pone.0330692.ref021" class="usa-link" aria-describedby="pone.0330692.ref021">21</a>].</p>
<p>ASR has undergone significant advancements since its inception. In 1952, Davis et al. [<a href="#pone.0330692.ref028" class="usa-link" aria-describedby="pone.0330692.ref028">28</a>] pioneered early speech recognition systems capable of recognizing digits, achieving notable accuracy despite the technological limitations of the time. Modern ASR systems, powered by AI and neural network architectures such as the Transformer model, have since revolutionized speech-to-text technology, enabling real-time transcription of large-scale audio data with greater accuracy and reliability [<a href="#pone.0330692.ref029" class="usa-link" aria-describedby="pone.0330692.ref029">29</a>]. However, accuracy in ASR remains a multifaceted challenge, particularly in real-world environments. This includes challenges in recognizing speech accurately in the same language (monolingual transcription) and auto-translating speech into another language (interlingual transcription) [<a href="#pone.0330692.ref009" class="usa-link" aria-describedby="pone.0330692.ref009">9</a>,<a href="#pone.0330692.ref016" class="usa-link" aria-describedby="pone.0330692.ref016">16</a>,<a href="#pone.0330692.ref017" class="usa-link" aria-describedby="pone.0330692.ref017">17</a>,<a href="#pone.0330692.ref030" class="usa-link" aria-describedby="pone.0330692.ref030">30</a>]. Factors such as background noise, accents, dialects, and linguistic variations further complicate recognition tasks [<a href="#pone.0330692.ref029" class="usa-link" aria-describedby="pone.0330692.ref029">29</a>,<a href="#pone.0330692.ref030" class="usa-link" aria-describedby="pone.0330692.ref030">30</a>]. These limitations are particularly critical in high-stakes settings such as live conferences, where errors in recognition or translation could lead to miscommunication or misunderstandings.</p>
<p>Despite these challenges, automatic subtitles have become an essential tool for facilitating accessibility. They are especially valuable for ensuring accessibility for deaf or hard of hearing individuals, as well as for native [<a href="#pone.0330692.ref031" class="usa-link" aria-describedby="pone.0330692.ref031">31</a>] and non-native speakers [<a href="#pone.0330692.ref032" class="usa-link" aria-describedby="pone.0330692.ref032">32</a>], including those in noisy or quiet environments, multitaskers, and others with specific needs or preferences. Platforms such as Zoom and Microsoft Teams have integrated auto-subtitling features as part of their services to address accessibility needs. Although these features are typically included in paid licenses, they are perceived as lower-cost alternatives to human-provided services, such as SI or professional subtitling, due to the scalability and automation of ASR technology [<a href="#pone.0330692.ref020" class="usa-link" aria-describedby="pone.0330692.ref020">20</a>]. Auto-subtitling might not be as effective as having SI because it lacks the nuances of spoken language that help keep people interested and oriented [<a href="#pone.0330692.ref033" class="usa-link" aria-describedby="pone.0330692.ref033">33</a>]. Therefore, some may prefer overcoming language barriers by listening to SI because the need to expend excessive cognitive effort to follow visually a narrative may result in a negative viewing experience [<a href="#pone.0330692.ref034" class="usa-link" aria-describedby="pone.0330692.ref034">34</a>].</p>
<p>Research to date has focused on the impact of automatic subtitles on interpreters themselves [<a href="#pone.0330692.ref009" class="usa-link" aria-describedby="pone.0330692.ref009">9</a>,<a href="#pone.0330692.ref021" class="usa-link" aria-describedby="pone.0330692.ref021">21</a>,<a href="#pone.0330692.ref022" class="usa-link" aria-describedby="pone.0330692.ref022">22</a>]. These discussions have largely emphasized how auto-subtitling can support interpreters by improving their accuracy and reducing cognitive load [<a href="#pone.0330692.ref035" class="usa-link" aria-describedby="pone.0330692.ref035">35</a>]. However, listeners can also benefit from the integration of automatic subtitles. In virtual conferences, where there is a need to accommodate diverse linguistic requirements and ensure accessibility for a broader audience, automatic subtitles can play a crucial role in enhancing the delivery of multilingual content. This is particularly important in settings with participants from varied linguistic and cognitive backgrounds, where balancing visual and auditory inputs is essential for optimizing information accessibility [<a href="#pone.0330692.ref010" class="usa-link" aria-describedby="pone.0330692.ref010">10</a>].</p></section><section id="sec004"><h3 class="pmc_sec_title">Measuring cognitive effort and viewing experience using EEG data</h3>
<p>As auto-subtitling systems become increasingly prevalent across various contexts [<a href="#pone.0330692.ref003" class="usa-link" aria-describedby="pone.0330692.ref003">3</a>,<a href="#pone.0330692.ref021" class="usa-link" aria-describedby="pone.0330692.ref021">21</a>,<a href="#pone.0330692.ref022" class="usa-link" aria-describedby="pone.0330692.ref022">22</a>,<a href="#pone.0330692.ref036" class="usa-link" aria-describedby="pone.0330692.ref036">36</a>], understanding their impact on viewers’ cognitive performance and comprehension is crucial for optimizing communication and reducing language barriers [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>]. The concepts of cognitive effort and stress may appear similar but are in fact distinct. Cognitive effort is typically defined as the mental resources allocated for processing a task [<a href="#pone.0330692.ref036" class="usa-link" aria-describedby="pone.0330692.ref036">36</a>], whereas stress refers to a particular relationship between an individual and their environment in which the demands of the situation are perceived as taxing or exceeding the individual’s resources, potentially affecting their focus and functional performance [<a href="#pone.0330692.ref037" class="usa-link" aria-describedby="pone.0330692.ref037">37</a>]. In the context of viewers’ audiovisual content comprehension, cognitive effort relates to the mental resources required to process information when viewing automatic subtitles and simultaneously listening to spoken content [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>]. Stress, on the other hand, arises when the cognitive demands of processing audiovisual information are perceived as overwhelming or exceeding an individual’s comfort level [<a href="#pone.0330692.ref037" class="usa-link" aria-describedby="pone.0330692.ref037">37</a>]. This can affect viewers’ ability to maintain focus and process content effectively, especially in high-pressure environments such as live conferences. Research into viewers’ cognitive performance in audiovisual content comprehension, particularly in the context of automatic subtitles, represents a vital area of study. It investigates the relationship between multimodal content processing and cognitive functions, providing valuable insights into how technologies like auto-subtitling can either support or obstruct comprehension, ultimately shaping the effectiveness of information delivery.</p>
<p>To effectively measure cognitive effort and stress, researchers commonly utilize both subjective and objective assessment methods. Subjective measures, such as self-report questionnaires, enable participants to describe their perceived levels of cognitive effort and stress during specific tasks [<a href="#pone.0330692.ref038" class="usa-link" aria-describedby="pone.0330692.ref038">38</a>,<a href="#pone.0330692.ref039" class="usa-link" aria-describedby="pone.0330692.ref039">39</a>]. For instance, self-reported data in [<a href="#pone.0330692.ref040" class="usa-link" aria-describedby="pone.0330692.ref040">40</a>] suggested that computer-assisted consecutive interpreting resulted in fewer pauses, reduced cognitive load, and improved overall interpreting quality, particularly in terms of accuracy. However, conflicting findings have emerged. The conflict matrix, which lacks “reliable empirical data to substantiate” its claims [<a href="#pone.0330692.ref041" class="usa-link" aria-describedby="pone.0330692.ref041">41</a>], suggests that while additional information might logically enhance interpretation accuracy, comparisons between cognitive resource allocation and the conflict matrix for SI with and without text predict a discrete increase in task interference, and consequently, cognitive load. These conflicting findings may, in part, be attributed to the limited accuracy and reliability of subjective measures [<a href="#pone.0330692.ref041" class="usa-link" aria-describedby="pone.0330692.ref041">41</a>].</p>
<p>Objective measures offer a detailed understanding of cognitive effort and stress by tracking real-time physiological responses linked to cognitive processes. These responses usually involve the central and peripheral nervous systems [<a href="#pone.0330692.ref042" class="usa-link" aria-describedby="pone.0330692.ref042">42</a>]. The most common way to accurately decode the rhythm of the nervous system is to use EEG data [<a href="#pone.0330692.ref043" class="usa-link" aria-describedby="pone.0330692.ref043">43</a>], which enables researchers to correlate specific brain responses with performance metrics. In recent years, advancements in human–computer interaction and the increased accessibility of low-cost consumer-grade EEG devices have led to widespread recognition within research communities that objective EEG data provide a robust and reliable means of measuring participants’ cognitive performance and the user experience [<a href="#pone.0330692.ref009" class="usa-link" aria-describedby="pone.0330692.ref009">9</a>,<a href="#pone.0330692.ref044" class="usa-link" aria-describedby="pone.0330692.ref044">44</a>–<a href="#pone.0330692.ref047" class="usa-link" aria-describedby="pone.0330692.ref047">47</a>].</p>
<p>Analyzing brain wave patterns is essential for understanding cognitive effort and stress levels [<a href="#pone.0330692.ref045" class="usa-link" aria-describedby="pone.0330692.ref045">45</a>]. While the relationship between alpha wave fluctuations and cognitive workload remains somewhat unclear [<a href="#pone.0330692.ref048" class="usa-link" aria-describedby="pone.0330692.ref048">48</a>], neural oscillations associated with theta and beta waves are recognized as reliable markers of cognitive workload [<a href="#pone.0330692.ref049" class="usa-link" aria-describedby="pone.0330692.ref049">49</a>]. According to previous studies [<a href="#pone.0330692.ref042" class="usa-link" aria-describedby="pone.0330692.ref042">42</a>,<a href="#pone.0330692.ref050" class="usa-link" aria-describedby="pone.0330692.ref050">50</a>], theta waves (4–8 Hz) are associated with memory processing and cognitive load; increased theta activity may indicate significant cognitive effort and greater strain on working memory [<a href="#pone.0330692.ref050" class="usa-link" aria-describedby="pone.0330692.ref050">50</a>] and increased mental fatigue [<a href="#pone.0330692.ref051" class="usa-link" aria-describedby="pone.0330692.ref051">51</a>–<a href="#pone.0330692.ref053" class="usa-link" aria-describedby="pone.0330692.ref053">53</a>]. In such a setting, theta power can indicate mental resource allocation during multimodal (audiovisual) content processing. Higher theta activity in the frontal region corresponds to executive functions, including decision-making, working memory, and semantic processing, while occipital region activity reflects visual processing and reading comprehension [<a href="#pone.0330692.ref054" class="usa-link" aria-describedby="pone.0330692.ref054">54</a>]. Regarding beta waves, Lazarus and Folkman [<a href="#pone.0330692.ref055" class="usa-link" aria-describedby="pone.0330692.ref055">55</a>] identified elevated beta wave power as an indicator of increased stress levels, as beta waves are associated with heightened cognitive activity and emotional strain during task performance. In particular, high beta wave activity often reflects mental states characterized by tension, anxiety, or demands associated with sustaining attention under pressure. These findings are further supported by research in interpreting and language translation contexts, which has used beta wave activity to measure cognitive and emotional strain [<a href="#pone.0330692.ref051" class="usa-link" aria-describedby="pone.0330692.ref051">51</a>,<a href="#pone.0330692.ref052" class="usa-link" aria-describedby="pone.0330692.ref052">52</a>].</p>
<p>The adoption of EEG technology has been instrumental in observing the cognitive mechanisms of simultaneous interpreters. Early studies by Kurz [<a href="#pone.0330692.ref056" class="usa-link" aria-describedby="pone.0330692.ref056">56</a>,<a href="#pone.0330692.ref057" class="usa-link" aria-describedby="pone.0330692.ref057">57</a>] explored EEG changes during SI, providing valuable insights into the mental processes underlying complex verbal tasks. These studies also confirmed the utility of computer-assisted neurophysiological measures for investigating cortical processes during SI. Building on this foundation, Koshkin et al. [<a href="#pone.0330692.ref058" class="usa-link" aria-describedby="pone.0330692.ref058">58</a>] employed EEG techniques to examine interpreters’ neural activity under varying levels of working memory load during the SI of continuous prose. Similarly, Moser-Mercer [<a href="#pone.0330692.ref059" class="usa-link" aria-describedby="pone.0330692.ref059">59</a>] underscored the importance of studying cognitive functioning in simultaneous interpreters, highlighting the “plasticity” of the interpreter’s brain and how experienced professionals can overcome common cognitive constraints through expertise. Yagura et al. [<a href="#pone.0330692.ref060" class="usa-link" aria-describedby="pone.0330692.ref060">60</a>] further advanced this area by quantifying differences in brain activity patterns between experienced and novice interpreters using EEGs in realistic SI environments. Their findings revealed that years of SI experience significantly influence selective attention during interpretation.</p>
<p>While existing studies primarily focus on the use of EEG techniques to investigate simultaneous interpreters’ cognitive processes, research on viewers’ cognitive performance under different interpreting modes remains sparse. Specifically, the combined effects of auto-subtitling systems and SI on viewers’ cognitive performance have yet to be thoroughly explored. Given the growing emphasis on leveraging advanced technologies to enhance information accessibility, it is crucial to examine how these modes interact. This knowledge gap is particularly significant in light of the increasing integration of technology into interpreting contexts. Addressing this gap is essential not only for advancing theoretical understanding of interpreting practices but also for developing evidence-based approaches to improve interpreting performance and enhance user satisfaction in multilingual communication settings.</p></section></section><section id="sec005"><h2 class="pmc_sec_title">Method</h2>
<p>This study adopted a mixed-methods approach to examine participants’ cognitive performance and comprehension. Participants’ comprehension was assessed using an online questionnaire, while an EEG device measured their cognitive performance.</p>
<p>Ethics approval was obtained from The Hong Kong Polytechnic University Institutional Review Board (PolyU IRB, reference number HSEARS20241022005, 28 October 2024). Participants, recruited via offline classes, provided oral consent after reviewing a clear, non-technical information sheet outlining the study’s aims and procedures. They were assured of their right to withdraw at any time without penalty.</p>
<section id="sec006"><h3 class="pmc_sec_title">Participants</h3>
<p>A total of 45 participants (28 female, 17 male) took part in the study, with 15 individuals in each of the three experimental groups (see <a href="#pone.0330692.t001" class="usa-link">Table 1</a> for group-specific demographics). All participants were native speakers of Mandarin Chinese and reported using Mandarin as their primary language of communication. Self-assessed proficiency aligned with the C2 level of the Common European Framework of Reference for Languages (CEFR). Screening questions confirmed no history of visual, auditory, or language-related impairments.</p>
<section class="tw xbox font-sm" id="pone.0330692.t001"><h4 class="obj_head">Table 1. Demographic characteristics of participants (N = 45).</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Demographic Characteristics</th>
<th align="left" rowspan="1" colspan="1">Total<br> (N = 45)</th>
<th align="left" rowspan="1" colspan="1">Group A</th>
<th align="left" rowspan="1" colspan="1">Group B</th>
<th align="left" rowspan="1" colspan="1">Group C</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Female</td>
<td align="left" rowspan="1" colspan="1">28</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">9</td>
<td align="left" rowspan="1" colspan="1">9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Male</td>
<td align="left" rowspan="1" colspan="1">17</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Average Age</td>
<td align="left" rowspan="1" colspan="1">23.7</td>
<td align="left" rowspan="1" colspan="1">23.5</td>
<td align="left" rowspan="1" colspan="1">23.6</td>
<td align="left" rowspan="1" colspan="1">23.9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Age Range</td>
<td align="left" rowspan="1" colspan="1">21-26</td>
<td align="left" rowspan="1" colspan="1">21-25</td>
<td align="left" rowspan="1" colspan="1">21-26</td>
<td align="left" rowspan="1" colspan="1">21-26</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330692.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Participants were born and raised in mainland China, with representation from multiple provinces and municipalities, including Beijing, Shanghai, Guangdong, and Sichuan. They were recruited from a range of academic disciplines to ensure variation in educational background and to minimize domain-specific bias. None of the participants reported any academic or professional background in fields such as international relations, diplomacy, or United Nations (UN) affairs, based on the information gathered through background questionnaires. Although participants were not directly asked about their prior familiarity with the topic of the stimulus video, their disciplinary profiles and lack of relevant training suggest limited exposure to the domain. This helps to mitigate the influence of domain expertise on task performance and ensures that the study primarily captures general audience comprehension.</p>
<p>To examine the effects of different content delivery methods on cognitive effort, viewing experience, and comprehension, the 45 participants were randomly assigned to three experimental groups of equal size (n = 15): Group A (automatic subtitles), Group B (SI), and Group C (a combination of automatic subtitles and SI).</p>
<p>While stratified randomization was not employed at the assignment stage, this decision was informed by the modest sample size and logistical considerations. Given that demographic variables were not expected to exert strong effects on the dependent measures, simple randomization was deemed sufficient. Post-allocation checks confirmed satisfactory balance across key participant characteristics. Gender distribution was comparable across groups (Group A: 10F/5M, Group B: 9F/6M, Group C: 9F/6M), and participant ages ranged from 21 to 26 years, with group averages between 23.5 and 23.9 (see <a href="#pone.0330692.t001" class="usa-link">Table 1</a>). Participants were also diverse in their academic backgrounds, with no concentration of disciplinary expertise in any one group. These checks support the internal validity of the group comparisons and help rule out confounding effects from participant-level variables.</p>
<p>The formal experiment commenced on November 1 and concluded on November 18, 2024. All participants wore EEG devices while watching the conference video in individual rooms without any intervention. We monitored their EEG signals from a separate independent room. To control for potential stress-inducing factors, the experimental environment was designed to be quiet and low-pressure. Tasks were untimed, and no performance-based feedback was provided. Participants were encouraged to complete the questionnaire at their own pace. Baseline measures of cognitive state were also collected to account for individual differences in initial stress levels. Informal debriefings indicated that participants generally did not perceive the task as stressful. These measures help ensure that any observed effects are attributable to the experimental manipulation.</p></section><section id="sec007"><h3 class="pmc_sec_title">EEG headset and key brain waves</h3>
<p>The Emotiv EPOC X 14-channel wireless EEG headset was used to record brain activity, with raw EEG data accessed through the Emotiv Pro Suite installed on an HP Pavilion Plus laptop. The headset was paired with Emotiv Pro via Bluetooth, enabling effective real-time monitoring of EEG data. The Emotiv headset provides the power of five frequency bands, namely theta (4–8 Hz), alpha (8–12 Hz), low beta L (12–16 Hz), high beta H (16–25 Hz), and gamma (25–45 Hz), for each sensor, as shown in <a href="#pone.0330692.g001" class="usa-link">Fig 1</a>.</p>
<figure class="fig xbox font-sm" id="pone.0330692.g001"><h4 class="obj_head">Fig 1. Electrode placement of the Emotiv Epoc X EEG headset.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373249_pone.0330692.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8452/12373249/681eb17f7f10/pone.0330692.g001.jpg" loading="lazy" height="420" width="770" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330692.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>AF: anterior–frontal, F: frontal, FC: fronto-central, T: temporal, P: parietal, O: occipital. The odd numbers denote the left hemisphere and even numbers represent the right hemisphere. Black dots indicate locations of reference electrodes.</p></figcaption></figure></section><section id="sec008"><h3 class="pmc_sec_title">Materials</h3>
<p>A single speech in Arabic delivered at a UN conference was selected as the experimental material. This speech was chosen because it reflects the UN’s standard practice of providing SI services into its six official languages, aligning with our focus on evaluating comprehension of interpreted content in professional settings. The original speech was interpreted into Chinese by professional UN interpreters, and the official video and audio recordings were publicly available on the UN website. To ensure experimental control, all participants across the three groups were exposed to identical speech content, which was then processed into three distinct delivery modes:</p>
<ul class="list" style="list-style-type:none">
<li><p>Version A: Chinese auto-subtitling generated by an ASR system based on the Chinese SI audio.</p></li>
<li><p>Version B: Chinese SI audio only, without subtitles.</p></li>
<li><p>Version C: A combination of Chinese SI audio and Chinese automatic subtitles.</p></li>
</ul>
<p>The speech content, pacing, and audiovisual quality were held constant across all three versions. Each version lasted exactly 3 minutes and 40 seconds. The use of a single speech across all experimental conditions was an intentional design choice aimed at maximizing internal validity. This design ensured that the only manipulated variable was the mode of delivery, while the linguistic content, discourse topic, and communicative context remained constant. Using a single speech allowed us to isolate the effects of delivery mode on cognitive effort and comprehension outcomes without introducing variability from message content or situational features.</p>
<p>To assess the participants’ comprehension of the speech in the target language, a multiple-choice test was developed in Simplified Chinese characters. The test consisted of 10 single-choice questions focused on factual recall, such as the speech’s main topics, numerical data, and named entities, to evaluate the participants’ immediate understanding of the content. An example of a question translated in English is as follows:</p>
<ul class="list" style="list-style-type:none">
<li><p>“<em>What is the purpose of holding this summit?</em></p></li>
<li><p>A) To promote global economic recovery</p></li>
<li><p>B) To emphasize national sovereignty and integrity</p></li>
<li><p>C) To enhance economic cooperation among countries</p></li>
<li><p>D) To reshape the multilateral system”</p></li>
<li><p>Correct answer:</p></li>
<li><p>D) This option directly reflects the central goal as stated in the speech: “reshape the multilateral system”.</p></li>
<li><p>Distractor rationale:</p></li>
<li><p>A) Reflects general UN themes related to development and recovery but does not align with the summit’s explicitly stated focus.</p></li>
<li><p>B) Draws on the principle of respecting sovereignty mentioned in the speech but misrepresents it as the main purpose.</p></li>
<li><p>C) Suggests international cooperation, a peripheral theme, but inaccurately elevates it to the summit’s core objective.</p></li>
</ul>
<p>Each question was worth 10 points, with no partial credit awarded for incorrect or incomplete answers. As a result, the total comprehension score ranged from 0 to 100, with higher scores indicating better comprehension. To ensure the test’s validity and reliability, all questions and answer options were reviewed by a linguistics expert who specialized in translation and interpreting. This review ensured that the questions aligned with the speech content and that the wording of the answer options was clear, precise, and appropriate for assessing comprehension of the target language.</p></section><section id="sec009"><h3 class="pmc_sec_title">Experimental procedure</h3>
<p>A pilot test was conducted prior to the main study to ensure the feasibility of the experimental design and procedures. This included evaluating the functionality of video playback, the clarity of the instructions, and the timing of the comprehension test. Feedback from the pilot phase informed minor adjustments, including the rewording of comprehension questions to improve clarity and technical refinements to ensure the smooth functioning of all experimental components. One such revision involved replacing the original item <em>“Why was the summit important?”</em>, which invited subjective interpretation, with <em>“What is the purpose of holding this summit?”</em>, a fact-based question more directly anchored in the explicit linguistic content of the source text. The pilot confirmed the feasibility of the materials and procedures for the main experiment. Accordingly, the formal study adopted the validated setup—including standardized video playback procedures—and was conducted in a controlled lab environment to ensure consistency and data reliability.</p>
<p>Participants were randomly assigned to one of three experimental conditions—SI, automatic subtitles, or dual-modality (SI + automatic subtitles)—using a computer-generated randomization sequence. Each participant experienced only one modality condition and viewed the same stimulus video in a fixed order, followed by comprehension questions. Random allocation was employed to control for potential between-group variation unrelated to the experimental treatment.</p>
<p>Each participant selected a session time based on their availability and was informed in advance that they would receive a HK$50 gift card as compensation. Based on each participant’s selected schedule, the research team reserved the lab and ensured the appropriate condition materials were prepared. To minimize bias, participants were explicitly instructed not to discuss their experiences or the test content with one another until all sessions were completed.</p>
<p>Upon arrival, the participant was provided with a consent form to read and sign, ensuring ethical compliance with the study protocol. Following this, the participant was briefed on the experimental procedure and introduced to the wireless EEG device, which was used to monitor their cognitive activity. Each experimental session was conducted individually. A trained research assistant assisted the participant in correctly positioning the wireless EEG headset and ensured that 100% contact quality was achieved before proceeding with the task.</p>
<p>Once the EEG setup and baseline signal confirmation were complete, the participant was seated at an individual station and the video presentation commenced. During the video viewing, researchers monitored the participant from a separate room to ensure uninterrupted EEG data collection and to quickly address any technical issues. The environment was carefully controlled to eliminate distractions, such as noise or external interference, ensuring consistent testing conditions for each participant.</p>
<p>To maintain consistency, the video could not be paused or replayed during the experiment. The comprehension questions were only accessible after the video had ended, to ensure that the participant relied solely on the presented material. <a href="#pone.0330692.g002" class="usa-link">Fig 2</a> presents the participant wearing the EEG device while watching the video material.</p>
<figure class="fig xbox font-sm" id="pone.0330692.g002"><h4 class="obj_head">Fig 2. Participant wearing the EEG device while watching the video.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373249_pone.0330692.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8452/12373249/fa377a8ed6b9/pone.0330692.g002.jpg" loading="lazy" height="400" width="777" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330692.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section></section><section id="sec010"><h2 class="pmc_sec_title">Results</h2>
<section id="sec011"><h3 class="pmc_sec_title">Content comprehension</h3>
<p>After all participants completed the experiments, their comprehension scores were collected and processed for statistical analysis. To compare comprehension scores across the three groups, the Kruskal-Wallis test was employed. The analysis yielded an <em>H</em> statistic of 0.407 (<em>df</em> = 2, <em>p</em> = 0.816). As <em>p</em> <em>&gt;</em> 0.05, the null hypothesis of equal distributions across groups was not rejected. This result indicates that there were no statistically significant differences in comprehension scores among the three groups.</p>
<p>Descriptive statistics (<a href="#pone.0330692.t002" class="usa-link">Table 2</a>) show a gradual increase in mean scores: Group A (automatic subtitles) scored 70.00, Group B (SI) scored 74.00, and Group C (dual-modality) scored 75.33. Although the differences were statistically non-significant, this incremental pattern suggests potential benefits of dual-modality delivery for comprehension.</p>
<section class="tw xbox font-sm" id="pone.0330692.t002"><h4 class="obj_head">Table 2. Comprehension scores of the three groups.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Group</th>
<th align="left" rowspan="1" colspan="1">Max</th>
<th align="left" rowspan="1" colspan="1">Min</th>
<th align="left" rowspan="1" colspan="1">Mode</th>
<th align="left" rowspan="1" colspan="1">Mean</th>
<th align="left" rowspan="1" colspan="1">SD</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Group A</td>
<td align="left" rowspan="1" colspan="1">100</td>
<td align="left" rowspan="1" colspan="1">20</td>
<td align="left" rowspan="1" colspan="1">90</td>
<td align="left" rowspan="1" colspan="1">70.00</td>
<td align="left" rowspan="1" colspan="1">23.30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Group B</td>
<td align="left" rowspan="1" colspan="1">100</td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">80</td>
<td align="left" rowspan="1" colspan="1">74.00</td>
<td align="left" rowspan="1" colspan="1">15.95</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Group C</td>
<td align="left" rowspan="1" colspan="1">100</td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">90</td>
<td align="left" rowspan="1" colspan="1">75.33</td>
<td align="left" rowspan="1" colspan="1">18.85</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330692.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="t002fn001"><p>Notes: Group A: Automatic subtitles; Group B: SI; Group C: Automatic subtitles + SI.</p></div></div></section><p>Group A exhibited the highest variability (<em>SD</em> = 23.30), with scores ranging from 20 to 100, compared to narrower ranges in Group B (40–100) and Group C (30–100). Notably, all groups included participants achieving perfect scores (100), indicating that each modality could support optimal comprehension under favorable conditions. Group C, however, combined the highest mean score with a relatively homogeneous distribution (<em>SD</em> = 18.85), suggesting enhanced stability in information processing.</p></section><section id="sec012"><h3 class="pmc_sec_title">Cognitive effort</h3>
<p>Participants’ neurological responses to three distinct content delivery modes were investigated through analysis of brain activity patterns across participant groups. The topographical distribution of the raw EEG data was visualized using the EEGLab toolbox [<a href="#pone.0330692.ref061" class="usa-link" aria-describedby="pone.0330692.ref061">61</a>] in Matlab R2022a, as illustrated in <a href="#pone.0330692.g003" class="usa-link">Figs 3</a> and <a href="#pone.0330692.g004" class="usa-link">4</a>. The analysis revealed distinct patterns of neural activity across the three groups, with particular emphasis on power bands, which are strongly associated with human cognitive performance.</p>
<figure class="fig xbox font-sm" id="pone.0330692.g003"><h4 class="obj_head">Fig 3. Topographic brain activity heatmap of EEG data in the theta band.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373249_pone.0330692.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8452/12373249/af272428c1c0/pone.0330692.g003.jpg" loading="lazy" height="193" width="731" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330692.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><figure class="fig xbox font-sm" id="pone.0330692.g004"><h4 class="obj_head">Fig 4. Topographic brain activity heatmap of EEG data in the beta bands.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373249_pone.0330692.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8452/12373249/614d7884e073/pone.0330692.g004.jpg" loading="lazy" height="426" width="764" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330692.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p><a href="#pone.0330692.g003" class="usa-link">Fig 3</a> presents the neural oscillations associated with theta waves, which have been recognized as reliable markers of cognitive effort [<a href="#pone.0330692.ref049" class="usa-link" aria-describedby="pone.0330692.ref049">49</a>], for the three groups. An increase in theta wave power is typically linked to tasks that demand greater cognitive effort, impose strain on working memory [<a href="#pone.0330692.ref050" class="usa-link" aria-describedby="pone.0330692.ref050">50</a>], or result in mental fatigue [<a href="#pone.0330692.ref053" class="usa-link" aria-describedby="pone.0330692.ref053">53</a>]. Moreover, theta waves serve as indicators of mental resource allocation during the processing of multimodal (audio-visual) content [<a href="#pone.0330692.ref051" class="usa-link" aria-describedby="pone.0330692.ref051">51</a>,<a href="#pone.0330692.ref052" class="usa-link" aria-describedby="pone.0330692.ref052">52</a>].</p>
<p>As shown in <a href="#pone.0330692.g003" class="usa-link">Fig 3</a>, Group B demonstrated the highest theta band activation in both the frontal region (AF3) and the occipital region (O7), followed by Group A, with lower theta band activation in the frontal region (AF3), and then Group C, with limited theta band activation.</p></section><section id="sec013"><h3 class="pmc_sec_title">Viewing experience</h3>
<p>Regarding the viewing experience, <a href="#pone.0330692.g004" class="usa-link">Fig 4</a> illustrates the hierarchical pattern of beta waves associated with active cognitive processing, problem-solving, and alertness, providing insights into the participants’ engagement levels and potential stress responses [<a href="#pone.0330692.ref060" class="usa-link" aria-describedby="pone.0330692.ref060">60</a>,<a href="#pone.0330692.ref062" class="usa-link" aria-describedby="pone.0330692.ref062">62</a>]. Elevated beta waves activity is generally correlated with excessive levels may indicate anxiety or stress [<a href="#pone.0330692.ref060" class="usa-link" aria-describedby="pone.0330692.ref060">60</a>,<a href="#pone.0330692.ref062" class="usa-link" aria-describedby="pone.0330692.ref062">62</a>]. Results show that Group A exhibited the highest beta band activation, indicating the highest stress level, followed by Group B, with Group C showing the lowest level of stress.</p>
<p>In addition to the raw EEG data, which were subsequently drawn as a topographic brain activity heatmap, the Emotiv Pro Suite software package was used to obtain stress values for the participants from the EEG devices while watching the 220-second conference video. We performed the non-parametric Kruskal-Wallis <em>H</em> test for the three groups, and the results are shown in <a href="#pone.0330692.g005" class="usa-link">Fig 5</a>. There were significant differences in the pressure values of the three groups <em>p</em> &lt; 0.01, demonstrating the accuracy and rationality of the discrepancies in the beta band.</p>
<figure class="fig xbox font-sm" id="pone.0330692.g005"><h4 class="obj_head">Fig 5. Non-parametric Kruskal-Wallis <em>H</em> test results (stress) for the three groups.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373249_pone.0330692.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8452/12373249/6d5e3f20f95e/pone.0330692.g005.jpg" loading="lazy" height="376" width="670" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330692.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section></section><section id="sec014"><h2 class="pmc_sec_title">Discussion</h2>
<section id="sec015"><h3 class="pmc_sec_title">How do automatic subtitles compare to SI in facilitating content comprehension for viewers?</h3>
<p>The experimental results revealed that the mode of delivery of translated information across the three groups did not significantly impact viewers’ comprehension scores. This aligns with findings by Szarkowska et al. [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>], who observed that the absence of audio input had only a modest effect on comprehension when subtitles were available. In addition, Liao et al. [<a href="#pone.0330692.ref063" class="usa-link" aria-describedby="pone.0330692.ref063">63</a>] found that when participants watched videos with subtitles but without accompanying audio, comprehension scores remained similar to those of participants who had access to both modalities. Other studies have further supported this observation, finding that even the non-syntactic segmentation of subtitles (i.e., splitting sentences at unnatural points) does not negatively impact viewers’ comprehension [<a href="#pone.0330692.ref064" class="usa-link" aria-describedby="pone.0330692.ref064">64</a>,<a href="#pone.0330692.ref065" class="usa-link" aria-describedby="pone.0330692.ref065">65</a>].</p>
<p>While our statistical analysis revealed only minor differences in comprehension scores across experimental conditions, the group exposed to a combination of automatic subtitles and SI achieved the highest average comprehension scores. This finding highlights the potential of multimodal translation delivery to enhance participants’ understanding [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>]. By distributing information across auditory and visual channels, the dual-modality approach allows participants to process content more efficiently, as cognitive resources can be shared between modalities, reducing the strain on any single processing channel.</p>
<p>This distribution aligns with the cognitive theory of multimedia learning [<a href="#pone.0330692.ref066" class="usa-link" aria-describedby="pone.0330692.ref066">66</a>,<a href="#pone.0330692.ref067" class="usa-link" aria-describedby="pone.0330692.ref067">67</a>], which suggests that synchronizing corresponding visual and auditory material reduces cognitive load. Subtitles can clarify ambiguous or unfamiliar auditory input, while auditory content provides additional context or emotional nuance that subtitles alone may not fully convey [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>]. The complementary nature of these modalities enhances understanding while minimizing the risk of cognitive overload, particularly in complex or fast-paced content. Consequently, multimodal delivery systems, such as the combination of automatic subtitles and SI, represent an effective and accessible strategy to support participants’ comprehension, especially in multilingual or cognitively demanding environments.</p></section><section id="sec016"><h3 class="pmc_sec_title">What differences in cognitive demands do automatic subtitles and SI impose on viewers?</h3>
<p>The experimental findings yielded important insights into cognitive effort during conference content processing across the three experimental groups. Analysis of EEG topographic brain activity indicated significant variations in cognitive effort between the SI (listening) and automatic subtitles (reading) conditions, offering valuable theoretical and practical implications.</p>
<p>The theta waves measurements demonstrated a hierarchical pattern of cognitive demand: Group B (SI only) exhibited the highest theta power, followed by Group A (automatic subtitles only), while Group C (combined SI and automatic subtitles) showed the lowest level. This finding is particularly relevant to conference interpreting, as conference attendees relying on translation services experience varying levels of cognitive load depending on the modality through which they receive translated information. The heightened cognitive effort in the SI-only condition suggests additional mental resources were needed for participants to comprehend by processing only auditory information while those listened to SI likely due to the mental effort required to process and comprehend auditory input in real time. In contrast, attendees using automatic subtitles only showed lower cognitive demand, as reading visual text may involve a less resource-intensive processing pathway. Most notably, attendees who had access to both SI and auto-subtitles, exhibited the lowest theta power, indicating reduced cognitive strain when provided with multimodal inputs. The minimal cognitive effort observed for Group C finds robust theoretical support from the bilingual dual-coding theory [<a href="#pone.0330692.ref068" class="usa-link" aria-describedby="pone.0330692.ref068">68</a>] that suggests memory performance improves when information is processed through multiple channels, in our case the visual and audio channels. These findings extend beyond simple cognitive load measurement, suggesting that the traditional reliance on audio-only SI may not fully optimize audience comprehension and engagement.</p>
<p>The comparison of Groups A and B reveals an interesting deviation from conventional assumptions that watching subtitle videos without corresponding audio would increase cognitive load as viewers use metacognitive strategies to adjust their visual processing to maintain comprehension [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>,<a href="#pone.0330692.ref069" class="usa-link" aria-describedby="pone.0330692.ref069">69</a>]. Audio input is transient and sequential, while visual input is often static and simultaneous, allowing viewers to process information at their own pace and revisit it as needed. Our finding may reflect the experimental setting, which simulated a conference setting where attendees typically listen to speakers for various purposes. In this context, participants were motivated to understand the experimental material, whether through auto-subtitles, SI, or a combination of both, potentially requiring additional cognitive resources for comprehension and retention. Our findings suggest that the professional context significantly may influence how different modalities affect cognitive processing.</p></section><section id="sec017"><h3 class="pmc_sec_title">How do automatic subtitles and SI influence viewers’ overall viewing experience?</h3>
<p>Beta wave analysis revealed distinct stress patterns across the three groups in relation to the viewing experience, providing insights on how different modes of translation delivery impact the overall viewing experience. Elevated beta activity is closely associated with increased mental workload and emotional tension, making it a reliable physiological marker for cognitive strain in audiovisual tasks [<a href="#pone.0330692.ref055" class="usa-link" aria-describedby="pone.0330692.ref055">55</a>]. Previous studies have employed beta wave analysis to assess user experiences in various applied contexts, such as game design and operational platform usability [<a href="#pone.0330692.ref046" class="usa-link" aria-describedby="pone.0330692.ref046">46</a>,<a href="#pone.0330692.ref047" class="usa-link" aria-describedby="pone.0330692.ref047">47</a>].</p>
<p>In this study, participants in Group A (automatic subtitles only) exhibited the highest beta wave levels, suggesting the greatest cognitive load, potentially diminishing subjective viewing comfort. This could be attributed to the demands of processing complex professional content solely through the visual modality. As Szarkowska et al. [<a href="#pone.0330692.ref019" class="usa-link" aria-describedby="pone.0330692.ref019">19</a>] observed, in the absence of auditory input in the first language (L1), viewers often adopt compensatory metacognitive strategies, such as reading subtitles more thoroughly, which can increase cognitive effort and stress.</p>
<p>Participants in Group B (SI only) showed lower beta wave levels compared to participants in Group A, suggesting a relatively smoother viewing experience. The real-time auditory input in SI likely facilitated reduced cognitive load by conveying prosodic features—such as tone, emotional emphasis, and pragmatic cues—that are inherently absent in text-based automatic subtitles. However, interpreting-based delivery is not without its own constraints. As prior studies have shown [<a href="#pone.0330692.ref070" class="usa-link" aria-describedby="pone.0330692.ref070">70</a>,<a href="#pone.0330692.ref071" class="usa-link" aria-describedby="pone.0330692.ref071">71</a>], the transient nature of spoken input places substantial demands on working memory, especially when the content is fast-paced or linguistically dense. Viewers must maintain high levels of attention, with no opportunity to pause, replay, or visually reinforce missed information.</p>
<p>Group C, which combined automatic subtitles and SI, demonstrated the lowest beta wave levels across all groups, suggesting the most cognitively relaxed and effective viewing experience. This dual-modality approach could balance cognitive load by leveraging the complementary strengths of both modalities. Automatic subtitles serve as a visual reinforcement, offering viewers extended exposure to verbal content that can support decoding or fill in missed auditory information. Meanwhile, SI enriches the experience by providing expressive and contextual nuances that subtitles may fail to capture. The redundancy in information delivery across modalities allows for greater processing flexibility: when one channel is momentarily ineffective or ambiguous, the other can compensate [<a href="#pone.0330692.ref066" class="usa-link" aria-describedby="pone.0330692.ref066">66</a>]. This integration not only reduces stress but also enhances comprehension and information retention.</p>
<p>These findings align with the Cognitive Theory of Multimedia Learning [<a href="#pone.0330692.ref066" class="usa-link" aria-describedby="pone.0330692.ref066">66</a>,<a href="#pone.0330692.ref067" class="usa-link" aria-describedby="pone.0330692.ref067">67</a>], which posits that when auditory and visual information are temporally aligned, cognitive resources are more efficiently distributed across dual channels. Such integration reduces the likelihood of overload in a single modality and enhances overall processing fluency. While our results do not confirm statistical significance in comprehension performance, the physiological evidence from EEG beta activity strongly suggests that dual-modality delivery promotes a more comfortable and cognitively sustainable viewing experience in demanding multilingual environments.</p></section></section><section id="sec018"><h2 class="pmc_sec_title">Practical implications</h2>
<p>The findings of this study offer several practical implications for reducing cognitive load and enhancing the usability of audiovisual content, particularly in contexts demanding SI and real-time accessibility. These insights are particularly relevant for online and hybrid conferences, where addressing the diverse needs of international audiences is critical to ensuring both engagement and comprehension.</p>
<p>First, this study reinforces the critical role of subtitle design in modulating cognitive load. Subtitle systems should not only prioritize linguistic accuracy but also be designed for cognitive accessibility, especially for non-native or linguistically disadvantaged users. In practice, this means ensuring optimal segmentation, timing, and readability, which can alleviate processing burden during complex or rapid content delivery. In multilingual classrooms or international live broadcasts, AI-powered subtitle systems, especially those informed by cognitive feedback metrics such as EEG or user-reported load [<a href="#pone.0330692.ref072" class="usa-link" aria-describedby="pone.0330692.ref072">72</a>,<a href="#pone.0330692.ref073" class="usa-link" aria-describedby="pone.0330692.ref073">73</a>], can support more inclusive and cognitively sustainable communication.</p>
<p>Second, the superior performance of the combined modality (SI with automatic subtitles) highlights the value of multimodal delivery in supporting comprehension and reducing stress. In specialized or technical conference settings, where terminology density and speech rate are often high, subtitles act as a visual scaffold, while SI provides prosodic and emotional cues that enhance contextual understanding. This dual-modality approach distributes processing demands across auditory and visual channels, thereby reducing cognitive overload and improving the overall viewer experience.</p>
<p>Moreover, although comprehension scores did not differ significantly across conditions, the dual-modality group exhibited both lower stress levels and the highest mean performance. These findings suggest that scalable, technology-driven multimodal systems can enhance cognitive resilience and flexibility. In cost-sensitive scenarios or instances where SI is unavailable, auto-subtitling may serve as a provisional alternative, provided that accuracy and synchronization are maintained. With continued advancements in AI and user-centered design, such systems can be refined to meet the increasing demand for inclusive and efficient multilingual communication in professional and globalized settings.</p></section><section id="sec019"><h2 class="pmc_sec_title">Conclusions</h2>
<p>This study investigates the combined effects of auto-subtitling and SI on viewers’ comprehension, cognitive effort, and overall viewing experience in a conference interpreting setting. The results showed that integrating automatic subtitles with SI reduced cognitive effort and improved the overall viewing experience compared to either modality alone, though comprehension differences were not statistically significant. EEG data revealed that participants in the automatic subtitle-only condition (Group A) exhibited the highest beta activity, followed by Group B (SI-only), while the dual-modality condition (Group C) showed the lowest levels of stress. Interestingly, despite the greater cognitive effort, participants in Group B rated their viewing experience more positively than those in Group A, suggesting that auditory input may provide additional engagement and relaxation.</p>
<p>Despite these promising results, our study has several limitations. These include a relatively small participant group, a short speech duration, and a focus on a single language pair, which may limit the generalizability of the findings. Future research should address these limitations by incorporating larger and more diverse samples, testing longer and more complex materials, and evaluating dual-modality systems across a broader range of language pairs and cultural contexts. In terms of system development, future research can build on these neurocognitive insights to explore adaptive technologies that integrate real-time EEG monitoring with AI-powered subtitle systems. Drawing inspiration from frameworks such as SUMART [<a href="#pone.0330692.ref072" class="usa-link" aria-describedby="pone.0330692.ref072">72</a>] and ClinClip’s EEG-informed modeling [<a href="#pone.0330692.ref073" class="usa-link" aria-describedby="pone.0330692.ref073">73</a>], future systems could dynamically adjust subtitle presentation parameters, such as speed, segmentation, and lexical complexity, based on ongoing cognitive feedback. Incorporating large language models (LLMs) with neurophysiological inputs, as shown in recent studies on personalized language learning [<a href="#pone.0330692.ref074" class="usa-link" aria-describedby="pone.0330692.ref074">74</a>,<a href="#pone.0330692.ref075" class="usa-link" aria-describedby="pone.0330692.ref075">75</a>], may further support real-time user-centered adaptation. Finally, future work should also consider extending accessibility beyond audio-visual modes by incorporating sign language interpreting.</p></section><section id="sec020"><h2 class="pmc_sec_title">Supporting information</h2>
<section class="sm xbox font-sm" id="pone.0330692.s001"><div class="caption p">
<span>S1 File. Questionnaire in EN.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s001.docx</a><sup> (11.9KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0330692.s002"><div class="caption p">
<span>S2 File. EEG based data.</span><p>(XLSX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s002.xlsx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s002.xlsx</a><sup> (44.3KB, xlsx) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0330692.s003"><div class="caption p">
<span>S3 File. Comprehension data.</span><p>(XLSX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s003.xlsx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s003.xlsx</a><sup> (10.2KB, xlsx) </sup>
</div></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>All relevant data are within the manuscript and its <a href="#sec020" class="usa-link">Supporting information</a> files.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>We would like to thank the Research and Innovation Office of the Hong Kong Polytechnic University for supporting the project (Project Code: RKQY). This manuscript was also partly supported by funding from Project number P0046386 of the Department of Chinese and Bilingual Studies of the Hong Kong Polytechnic University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0330692.ref001">
<span class="label">1.</span><cite>Cintas JD. Subtitling: Theory, practice and research. In: The Routledge handbook of translation studies. Routledge;
2013. p. 273–87.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20Routledge%20handbook%20of%20translation%20studies&amp;author=JD%20Cintas&amp;publication_year=2013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref002">
<span class="label">2.</span><cite>Cintas JD, Remael A. Subtitling: Concepts and practices. Routledge;
2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Subtitling:%20Concepts%20and%20practices&amp;author=JD%20Cintas&amp;author=A%20Remael&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref003">
<span class="label">3.</span><cite>Basari A, Nugroho RA. The use of Aegisub in teaching audiovisual translation classes: a review on IT-based subtitling course. In: Proceedings Education and Language International Conference, vol. 1, no. 1.
2017.</cite> [<a href="https://scholar.google.com/scholar_lookup?Basari%20A,%20Nugroho%20RA.%20The%20use%20of%20Aegisub%20in%20teaching%20audiovisual%20translation%20classes:%20a%20review%20on%20IT-based%20subtitling%20course.%20In:%20Proceedings%20Education%20and%20Language%20International%20Conference,%20vol.%201,%20no.%201.%202017." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref004">
<span class="label">4.</span><cite>Orrego-Carmona D. Audiovisual translation and audience reception. In: The Routledge Handbook of Audiovisual Translation. Routledge;
2018. p. 367–82. doi: 10.4324/9781315717166-23</cite> [<a href="https://doi.org/10.4324/9781315717166-23" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=The%20Routledge%20Handbook%20of%20Audiovisual%20Translation&amp;author=D%20Orrego-Carmona&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref005">
<span class="label">5.</span><cite>Tardel A. Effort in semi-automatized subtitling processes: speech recognition and experience during transcription. J Audiovisual Translat. 2020;3(2):79–102.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Audiovisual%20Translat&amp;title=Effort%20in%20semi-automatized%20subtitling%20processes:%20speech%20recognition%20and%20experience%20during%20transcription&amp;author=A%20Tardel&amp;volume=3&amp;issue=2&amp;publication_year=2020&amp;pages=79-102&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref006">
<span class="label">6.</span><cite>Lunin M, Minaeva L. Translated Subtitles Language Learning Method: A New Practical Approach to Teaching English. Procedia Soc Behav Sci. 2015;199:268–75. doi: 10.1016/j.sbspro.2015.07.516</cite> [<a href="https://doi.org/10.1016/j.sbspro.2015.07.516" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Procedia%20Soc%20Behav%20Sci&amp;title=Translated%20Subtitles%20Language%20Learning%20Method:%20A%20New%20Practical%20Approach%20to%20Teaching%20English&amp;author=M%20Lunin&amp;author=L%20Minaeva&amp;volume=199&amp;publication_year=2015&amp;pages=268-75&amp;doi=10.1016/j.sbspro.2015.07.516&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref007">
<span class="label">7.</span><cite>Vanderplank R. ‘Effects of’ and ‘effects with’ captions: How exactly does watching a TV programme with same-language subtitles make a difference to language learners?
Lang Teach. 2016;49(2):235–50. doi: 10.1017/s0261444813000207</cite> [<a href="https://doi.org/10.1017/s0261444813000207" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Lang%20Teach&amp;title=%E2%80%98Effects%20of%E2%80%99%20and%20%E2%80%98effects%20with%E2%80%99%20captions:%20How%20exactly%20does%20watching%20a%20TV%20programme%20with%20same-language%20subtitles%20make%20a%20difference%20to%20language%20learners?&amp;author=R%20Vanderplank&amp;volume=49&amp;issue=2&amp;publication_year=2016&amp;pages=235-50&amp;doi=10.1017/s0261444813000207&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref008">
<span class="label">8.</span><cite>Jung H. Subtitles in South Korean entertainment television: Focused on a third-person effect. J Entertain Media Stud. 2016;2(1):78.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Entertain%20Media%20Stud&amp;title=Subtitles%20in%20South%20Korean%20entertainment%20television:%20Focused%20on%20a%20third-person%20effect&amp;author=H%20Jung&amp;volume=2&amp;issue=1&amp;publication_year=2016&amp;pages=78&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref009">
<span class="label">9.</span><cite>Li T, Chmiel A. Automatic subtitles increase accuracy and decrease cognitive load in simultaneous interpreting. Interpreting. 2024;26(2):253–81. doi: 10.1075/intp.00111.li</cite> [<a href="https://doi.org/10.1075/intp.00111.li" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Interpreting&amp;title=Automatic%20subtitles%20increase%20accuracy%20and%20decrease%20cognitive%20load%20in%20simultaneous%20interpreting&amp;author=T%20Li&amp;author=A%20Chmiel&amp;volume=26&amp;issue=2&amp;publication_year=2024&amp;pages=253-81&amp;doi=10.1075/intp.00111.li&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref010">
<span class="label">10.</span><cite>Julaiti K, Cheung NDY, Cheung AKF, Huang JYJ. Number training in simultaneous interpreting: A corpus-assisted longitudinal study. Interpret Soc. 2025;5(1):80–96. doi: 10.1177/27523810241285542</cite> [<a href="https://doi.org/10.1177/27523810241285542" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Interpret%20Soc&amp;title=Number%20training%20in%20simultaneous%20interpreting:%20A%20corpus-assisted%20longitudinal%20study&amp;author=K%20Julaiti&amp;author=NDY%20Cheung&amp;author=AKF%20Cheung&amp;author=JYJ%20Huang&amp;volume=5&amp;issue=1&amp;publication_year=2025&amp;pages=80-96&amp;doi=10.1177/27523810241285542&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref011">
<span class="label">11.</span><cite>Cheung AKF. Listeners’ perception of the quality of simultaneous interpreting and perceived dependence on simultaneous interpreting. Interpreting. 2022;24(1):38–58. doi: 10.1075/intp.00070.che</cite> [<a href="https://doi.org/10.1075/intp.00070.che" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Interpreting&amp;title=Listeners%E2%80%99%20perception%20of%20the%20quality%20of%20simultaneous%20interpreting%20and%20perceived%20dependence%20on%20simultaneous%20interpreting&amp;author=AKF%20Cheung&amp;volume=24&amp;issue=1&amp;publication_year=2022&amp;pages=38-58&amp;doi=10.1075/intp.00070.che&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref012">
<span class="label">12.</span><cite>Cheung AKF. Cognitive load in remote simultaneous interpreting: place name translation in two Mandarin variants. Humanit Soc Sci Commun. 2024;11(1):1238. doi: 10.1057/s41599-024-03767-y</cite> [<a href="https://doi.org/10.1057/s41599-024-03767-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Humanit%20Soc%20Sci%20Commun&amp;title=Cognitive%20load%20in%20remote%20simultaneous%20interpreting:%20place%20name%20translation%20in%20two%20Mandarin%20variants&amp;author=AKF%20Cheung&amp;volume=11&amp;issue=1&amp;publication_year=2024&amp;pages=1238&amp;doi=10.1057/s41599-024-03767-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref013">
<span class="label">13.</span><cite>Garza TJ. Evaluating the Use of Captioned Video Materials in Advanced Foreign Language Learning. Foreign Lang Annals. 1991;24(3):239–58. doi: 10.1111/j.1944-9720.1991.tb00469.x</cite> [<a href="https://doi.org/10.1111/j.1944-9720.1991.tb00469.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Foreign%20Lang%20Annals&amp;title=Evaluating%20the%20Use%20of%20Captioned%20Video%20Materials%20in%20Advanced%20Foreign%20Language%20Learning&amp;author=TJ%20Garza&amp;volume=24&amp;issue=3&amp;publication_year=1991&amp;pages=239-58&amp;doi=10.1111/j.1944-9720.1991.tb00469.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref014">
<span class="label">14.</span><cite>Danan M. Captioning and Subtitling: Undervalued Language Learning Strategies. Meta Journal des Traducteurs/Translators’ Journal. 2004;49(1):67–77. doi: 10.7202/009021ar</cite> [<a href="https://doi.org/10.7202/009021ar" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Meta%20Journal%20des%20Traducteurs/Translators%E2%80%99%20Journal&amp;title=Captioning%20and%20Subtitling:%20Undervalued%20Language%20Learning%20Strategies&amp;author=M%20Danan&amp;volume=49&amp;issue=1&amp;publication_year=2004&amp;pages=67-77&amp;doi=10.7202/009021ar&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref015">
<span class="label">15.</span><cite>Gant Guillory H. The Effects of Keyword Captions to Authentic French Video on Learner Comprehension. CALICO J. 1998;15(1–3):89–108. doi: 10.1558/cj.v15i1-3.89-108
</cite> [<a href="https://doi.org/10.1558/cj.v15i1-3.89-108" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=CALICO%20J&amp;title=The%20Effects%20of%20Keyword%20Captions%20to%20Authentic%20French%20Video%20on%20Learner%20Comprehension&amp;author=H%20Gant%20Guillory&amp;volume=15&amp;publication_year=1998&amp;pages=89-108&amp;doi=10.1558/cj.v15i1-3.89-108&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref016">
<span class="label">16.</span><cite>van der Zee T, Admiraal W, Paas F, Saab N, Giesbers B. Effects of Subtitles, Complexity, and Language Proficiency on Learning From Online Education Videos. J Media Psychol. 2017;29(1):18–30. doi: 10.1027/1864-1105/a000208</cite> [<a href="https://doi.org/10.1027/1864-1105/a000208" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Media%20Psychol&amp;title=Effects%20of%20Subtitles,%20Complexity,%20and%20Language%20Proficiency%20on%20Learning%20From%20Online%20Education%20Videos&amp;author=T%20van%20der%20Zee&amp;author=W%20Admiraal&amp;author=F%20Paas&amp;author=N%20Saab&amp;author=B%20Giesbers&amp;volume=29&amp;issue=1&amp;publication_year=2017&amp;pages=18-30&amp;doi=10.1027/1864-1105/a000208&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref017">
<span class="label">17.</span><cite>Kruger JL, Doherty S, Soto-Sanfiel MT. Original language subtitles: their effects on the native and foreign viewer. Sci J Media Educ. 2017;25(50):23–32.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20J%20Media%20Educ&amp;title=Original%20language%20subtitles:%20their%20effects%20on%20the%20native%20and%20foreign%20viewer&amp;author=JL%20Kruger&amp;author=S%20Doherty&amp;author=MT%20Soto-Sanfiel&amp;volume=25&amp;issue=50&amp;publication_year=2017&amp;pages=23-32&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref018">
<span class="label">18.</span><cite>Liao S, Kruger J-L, Doherty S. The impact of monolingual and bilingual subtitles on visual attention, cognitive load, and comprehension. J Special Trans. 2020;(33):70–98. doi: 10.26034/cm.jostrans.2020.549</cite> [<a href="https://doi.org/10.26034/cm.jostrans.2020.549" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Special%20Trans&amp;title=The%20impact%20of%20monolingual%20and%20bilingual%20subtitles%20on%20visual%20attention,%20cognitive%20load,%20and%20comprehension&amp;author=S%20Liao&amp;author=J-L%20Kruger&amp;author=S%20Doherty&amp;issue=33&amp;publication_year=2020&amp;pages=70-98&amp;doi=10.26034/cm.jostrans.2020.549&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref019">
<span class="label">19.</span><cite>Szarkowska A, Ragni V, Szkriba S, Black S, Orrego-Carmona D, Kruger J-L. Watching subtitled videos with the sound off affects viewers’ comprehension, cognitive load, immersion, enjoyment, and gaze patterns: A mixed-methods eye-tracking study. PLoS One. 2024;19(10):e0306251. doi: 10.1371/journal.pone.0306251

</cite> [<a href="https://doi.org/10.1371/journal.pone.0306251" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11458047/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39374196/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Watching%20subtitled%20videos%20with%20the%20sound%20off%20affects%20viewers%E2%80%99%20comprehension,%20cognitive%20load,%20immersion,%20enjoyment,%20and%20gaze%20patterns:%20A%20mixed-methods%20eye-tracking%20study&amp;author=A%20Szarkowska&amp;author=V%20Ragni&amp;author=S%20Szkriba&amp;author=S%20Black&amp;author=D%20Orrego-Carmona&amp;volume=19&amp;issue=10&amp;publication_year=2024&amp;pmid=39374196&amp;doi=10.1371/journal.pone.0306251&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref020">
<span class="label">20.</span><cite>Malakul S, Park I. The effects of using an auto-subtitle system in educational videos to facilitate learning for secondary school students: learning comprehension, cognitive load, and satisfaction. Smart Learn Environ. 2023;10(1):4. doi: 10.1186/s40561-023-00224-2

</cite> [<a href="https://doi.org/10.1186/s40561-023-00224-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9831372/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40477865/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Smart%20Learn%20Environ&amp;title=The%20effects%20of%20using%20an%20auto-subtitle%20system%20in%20educational%20videos%20to%20facilitate%20learning%20for%20secondary%20school%20students:%20learning%20comprehension,%20cognitive%20load,%20and%20satisfaction&amp;author=S%20Malakul&amp;author=I%20Park&amp;volume=10&amp;issue=1&amp;publication_year=2023&amp;pages=4&amp;pmid=40477865&amp;doi=10.1186/s40561-023-00224-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref021">
<span class="label">21.</span><cite>Defrancq B, Fantinuoli C. Automatic speech recognition in the booth: Assessment of system performance, interpreters’ performances and interactions in the context of numbers. Target. 2021;33(1):73–102.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Target&amp;title=Automatic%20speech%20recognition%20in%20the%20booth:%20Assessment%20of%20system%20performance,%20interpreters%E2%80%99%20performances%20and%20interactions%20in%20the%20context%20of%20numbers&amp;author=B%20Defrancq&amp;author=C%20Fantinuoli&amp;volume=33&amp;issue=1&amp;publication_year=2021&amp;pages=73-102&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref022">
<span class="label">22.</span><cite>Tammasrisawat P, Rangponsumrit N. The Use of ASR-CAI Tools and Their Impact on Interpreters’ Performance During Simultaneous Interpretation. New Voices Translat Stud. 2023;28(2).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=New%20Voices%20Translat%20Stud&amp;title=The%20Use%20of%20ASR-CAI%20Tools%20and%20Their%20Impact%20on%20Interpreters%E2%80%99%20Performance%20During%20Simultaneous%20Interpretation&amp;author=P%20Tammasrisawat&amp;author=N%20Rangponsumrit&amp;volume=28&amp;issue=2&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref023">
<span class="label">23.</span><cite>Baranowska K. Learning most with least effort: subtitles and cognitive load. ELT J. 2020;74(2):105–15. doi: 10.1093/elt/ccz060</cite> [<a href="https://doi.org/10.1093/elt/ccz060" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=ELT%20J&amp;title=Learning%20most%20with%20least%20effort:%20subtitles%20and%20cognitive%20load&amp;author=K%20Baranowska&amp;volume=74&amp;issue=2&amp;publication_year=2020&amp;pages=105-15&amp;doi=10.1093/elt/ccz060&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref024">
<span class="label">24.</span><cite>Díaz-Cintas J.
The name and nature of subtitling. In: Bogucki Ł, Deckert M, editors. The Palgrave handbook of audiovisual translation and media accessibility. Cham: Springer International Publishing;
2020. p. 149–71.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20Palgrave%20handbook%20of%20audiovisual%20translation%20and%20media%20accessibility&amp;author=J.%20D%C3%ADaz-Cintas&amp;author=%C5%81%20Bogucki&amp;author=M%20Deckert&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref025">
<span class="label">25.</span><cite>O’Sullivan C, Cornu JF. History of audiovisual translation. The Routledge handbook of audiovisual translation. New York: Routledge; 2019.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=History%20of%20audiovisual%20translation.%20The%20Routledge%20handbook%20of%20audiovisual%20translation&amp;author=C%20O%E2%80%99Sullivan&amp;author=JF%20Cornu&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref026">
<span class="label">26.</span><cite>Alharbi S, Alrazgan M, Alrashed A, Alnomasi T, Almojel R, Alharbi R, et al. Automatic Speech Recognition: Systematic Literature Review. IEEE Access. 2021;9:131858–76. doi: 10.1109/access.2021.3112535</cite> [<a href="https://doi.org/10.1109/access.2021.3112535" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Automatic%20Speech%20Recognition:%20Systematic%20Literature%20Review&amp;author=S%20Alharbi&amp;author=M%20Alrazgan&amp;author=A%20Alrashed&amp;author=T%20Alnomasi&amp;author=R%20Almojel&amp;volume=9&amp;publication_year=2021&amp;pages=131858-76&amp;doi=10.1109/access.2021.3112535&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref027">
<span class="label">27.</span><cite>Cintas JD, Remael A. Audiovisual translation: Subtitling. London: Routledge;
2007.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Audiovisual%20translation:%20Subtitling&amp;author=JD%20Cintas&amp;author=A%20Remael&amp;publication_year=2007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref028">
<span class="label">28.</span><cite>Davis KH, Biddulph R, Balashek S. Automatic Recognition of Spoken Digits. The J Acoust Soc Am. 1952;24(6):637–42. doi: 10.1121/1.1906946</cite> [<a href="https://doi.org/10.1121/1.1906946" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=The%20J%20Acoust%20Soc%20Am&amp;title=Automatic%20Recognition%20of%20Spoken%20Digits&amp;author=KH%20Davis&amp;author=R%20Biddulph&amp;author=S%20Balashek&amp;volume=24&amp;issue=6&amp;publication_year=1952&amp;pages=637-42&amp;doi=10.1121/1.1906946&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref029">
<span class="label">29.</span><cite>Zhang Q, Lu H, Sak H, Tripathi A, McDermott E, Koo S, et al. Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss. In: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020. p. 4–8.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2020%20IEEE%20International%20Conference%20on%20Acoustics,%20Speech%20and%20Signal%20Processing%20(ICASSP)&amp;author=Q%20Zhang&amp;author=H%20Lu&amp;author=H%20Sak&amp;author=A%20Tripathi&amp;author=E%20McDermott&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref030">
<span class="label">30.</span><cite>Fujimoto M, Kawai H, One-pass single-channel noisy speech recognition using a combination of noisy and enhanced features. In: Interspeech Conference, Int Speech Commun A, editors. Graz, Austria: 2019.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Interspeech%20Conference,%20Int%20Speech%20Commun%20A&amp;author=M%20Fujimoto&amp;author=H%20Kawai&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref031">
<span class="label">31.</span><cite>Negi S, Mitra R. Native language subtitling of educational videos: A multimodal analysis with eye tracking, EEG and self‐reports. Brit J Educational Tech. 2022;53(6):1793–816. doi: 10.1111/bjet.13214</cite> [<a href="https://doi.org/10.1111/bjet.13214" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brit%20J%20Educational%20Tech&amp;title=Native%20language%20subtitling%20of%20educational%20videos:%20A%20multimodal%20analysis%20with%20eye%20tracking,%20EEG%20and%20self%E2%80%90reports&amp;author=S%20Negi&amp;author=R%20Mitra&amp;volume=53&amp;issue=6&amp;publication_year=2022&amp;pages=1793-816&amp;doi=10.1111/bjet.13214&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref032">
<span class="label">32.</span><cite>d’Ydewalle G, Praet C, Verfaillie K, Rensbergen JV. Watching Subtitled Television. Commun Res. 1991;18(5):650–66. doi: 10.1177/009365091018005005</cite> [<a href="https://doi.org/10.1177/009365091018005005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Commun%20Res&amp;title=Watching%20Subtitled%20Television&amp;author=G%20d%E2%80%99Ydewalle&amp;author=C%20Praet&amp;author=K%20Verfaillie&amp;author=JV%20Rensbergen&amp;volume=18&amp;issue=5&amp;publication_year=1991&amp;pages=650-66&amp;doi=10.1177/009365091018005005&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref033">
<span class="label">33.</span><cite>Gorman BM, Crabb M, Armstrong M. Adaptive subtitles: Preferences and trade-offs in real-time media adaption. In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. Yokohama, Japan: Association for Computing Machinery; 2021. p. 733.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%202021%20CHI%20Conference%20on%20Human%20Factors%20in%20Computing%20Systems&amp;author=BM%20Gorman&amp;author=M%20Crabb&amp;author=M%20Armstrong&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref034">
<span class="label">34.</span><cite>Becerra H, Ragano A, Debnath D, Ullah A, Lucas CR, Walsh M, et al. Dialogue understandability: Why are we streaming movies with subtitles? </cite>
</li>
<li id="pone.0330692.ref035">
<span class="label">35.</span><cite>Böser U, Deane-Cox S. Translation, interpreting and technological change: Innovations in research, practice and training. London: Bloomsbury Publishing;
2024.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Translation,%20interpreting%20and%20technological%20change:%20Innovations%20in%20research,%20practice%20and%20training&amp;author=U%20B%C3%B6ser&amp;author=S%20Deane-Cox&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref036">
<span class="label">36.</span><cite>González ER, Braun S, Davitti E, Korybski T. The use of automatic speech recognition in cloud-based remote simultaneous interpreting (Doctoral dissertation). University of Surrey; 2024.</cite> [<a href="https://scholar.google.com/scholar_lookup?Gonz%C3%A1lez%20ER,%20Braun%20S,%20Davitti%20E,%20Korybski%20T.%20The%20use%20of%20automatic%20speech%20recognition%20in%20cloud-based%20remote%20simultaneous%20interpreting%20(Doctoral%20dissertation).%20University%20of%20Surrey;%202024." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref037">
<span class="label">37.</span><cite>Folkman S. Stress: Appraisal and coping. Encyclopedia of behavioral medicine. Cham: Springer International Publishing; 2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Stress:%20Appraisal%20and%20coping.%20Encyclopedia%20of%20behavioral%20medicine&amp;author=S%20Folkman&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref038">
<span class="label">38.</span><cite>Thoen A, Steyaert J, Alaerts K, Evers K, Van Damme T. A Systematic Review of Self-Reported Stress Questionnaires in People on the Autism Spectrum. Rev J Autism Dev Disord. 2023;10(2):295–318. doi: 10.1007/s40489-021-00293-4

</cite> [<a href="https://doi.org/10.1007/s40489-021-00293-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8475841/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34603935/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Rev%20J%20Autism%20Dev%20Disord&amp;title=A%20Systematic%20Review%20of%20Self-Reported%20Stress%20Questionnaires%20in%20People%20on%20the%20Autism%20Spectrum&amp;author=A%20Thoen&amp;author=J%20Steyaert&amp;author=K%20Alaerts&amp;author=K%20Evers&amp;author=T%20Van%20Damme&amp;volume=10&amp;issue=2&amp;publication_year=2023&amp;pages=295-318&amp;pmid=34603935&amp;doi=10.1007/s40489-021-00293-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref039">
<span class="label">39.</span><cite>Westbrook A, Braver TS. Cognitive effort: A neuroeconomic approach. Cogn Affect Behav Neurosci. 2015;15(2):395–415. doi: 10.3758/s13415-015-0334-y

</cite> [<a href="https://doi.org/10.3758/s13415-015-0334-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4445645/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25673005/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cogn%20Affect%20Behav%20Neurosci&amp;title=Cognitive%20effort:%20A%20neuroeconomic%20approach&amp;author=A%20Westbrook&amp;author=TS%20Braver&amp;volume=15&amp;issue=2&amp;publication_year=2015&amp;pages=395-415&amp;pmid=25673005&amp;doi=10.3758/s13415-015-0334-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref040">
<span class="label">40.</span><cite>Chen S, Kruger JL. The effectiveness of computer-assisted interpreting: A preliminary study based on English-Chinese consecutive interpreting. Translat Interpret Stud. 2023;18(3):399–420.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Translat%20Interpret%20Stud&amp;title=The%20effectiveness%20of%20computer-assisted%20interpreting:%20A%20preliminary%20study%20based%20on%20English-Chinese%20consecutive%20interpreting&amp;author=S%20Chen&amp;author=JL%20Kruger&amp;volume=18&amp;issue=3&amp;publication_year=2023&amp;pages=399-420&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref041">
<span class="label">41.</span><cite>Seeber KG. Multimodal processing in simultaneous interpreting. The handbook of translation and cognition. 2017.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Multimodal%20processing%20in%20simultaneous%20interpreting.%20The%20handbook%20of%20translation%20and%20cognition&amp;author=KG%20Seeber&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref042">
<span class="label">42.</span><cite>Das Chakladar D, Roy PP. Cognitive workload estimation using physiological measures: a review. Cogn Neurodyn. 2024;18(4):1445–65. doi: 10.1007/s11571-023-10051-3

</cite> [<a href="https://doi.org/10.1007/s11571-023-10051-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11297869/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39104683/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cogn%20Neurodyn&amp;title=Cognitive%20workload%20estimation%20using%20physiological%20measures:%20a%20review&amp;author=D%20Das%20Chakladar&amp;author=PP%20Roy&amp;volume=18&amp;issue=4&amp;publication_year=2024&amp;pages=1445-65&amp;pmid=39104683&amp;doi=10.1007/s11571-023-10051-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref043">
<span class="label">43.</span><cite>Aghajani H, Garbey M, Omurtag A. Measuring Mental Workload with EEG+fNIRS. Front Hum Neurosci. 2017;11:359. doi: 10.3389/fnhum.2017.00359

</cite> [<a href="https://doi.org/10.3389/fnhum.2017.00359" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5509792/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28769775/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Hum%20Neurosci&amp;title=Measuring%20Mental%20Workload%20with%20EEG+fNIRS&amp;author=H%20Aghajani&amp;author=M%20Garbey&amp;author=A%20Omurtag&amp;volume=11&amp;publication_year=2017&amp;pages=359&amp;pmid=28769775&amp;doi=10.3389/fnhum.2017.00359&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref044">
<span class="label">44.</span><cite>Cano S, Araujo N, Guzman C, Rusu C, Albiol-Perez S. Low-Cost Assessment of User eXperience Through EEG Signals. IEEE Access. 2020;8:158475–87. doi: 10.1109/access.2020.3017685</cite> [<a href="https://doi.org/10.1109/access.2020.3017685" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Low-Cost%20Assessment%20of%20User%20eXperience%20Through%20EEG%20Signals&amp;author=S%20Cano&amp;author=N%20Araujo&amp;author=C%20Guzman&amp;author=C%20Rusu&amp;author=S%20Albiol-Perez&amp;volume=8&amp;publication_year=2020&amp;pages=158475-87&amp;doi=10.1109/access.2020.3017685&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref045">
<span class="label">45.</span><cite>Ragni V. Physiology-based research in second language acquisition: New evidence for didactic audiovisual translation. In: Empirical studies in didactic audiovisual translation. Routledge. 2025. p. 7–25.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Empirical%20studies%20in%20didactic%20audiovisual%20translation&amp;author=V%20Ragni&amp;publication_year=2025&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref046">
<span class="label">46.</span><cite>Li Y, Po Tsang Y, Lee CKM, Han S. Integrating neurophysiological sensing and group-based multi-criteria decision-making for fourth-party logistics platform selection. Adv Eng Info. 2025;64:102968. doi: 10.1016/j.aei.2024.102968</cite> [<a href="https://doi.org/10.1016/j.aei.2024.102968" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Eng%20Info&amp;title=Integrating%20neurophysiological%20sensing%20and%20group-based%20multi-criteria%20decision-making%20for%20fourth-party%20logistics%20platform%20selection&amp;author=Y%20Li&amp;author=Y%20Po%20Tsang&amp;author=CKM%20Lee&amp;author=S%20Han&amp;volume=64&amp;publication_year=2025&amp;pages=102968&amp;doi=10.1016/j.aei.2024.102968&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref047">
<span class="label">47.</span><cite>Paranthaman PK, Bajaj N, Solovey N, Jennings D. Comparative evaluation of the eeg performance metrics and player ratings on the virtual reality games. In: 2021 IEEE Conference on Games (CoG). 2021. p. 17–20.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2021%20IEEE%20Conference%20on%20Games%20(CoG)&amp;author=PK%20Paranthaman&amp;author=N%20Bajaj&amp;author=N%20Solovey&amp;author=D%20Jennings&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref048">
<span class="label">48.</span><cite>Castro-Meneses LJ, Kruger J-L, Doherty S. Validating theta power as an objective measure of cognitive load in educational video. Education Tech Research Dev. 2020;68(1):181–202. doi: 10.1007/s11423-019-09681-4</cite> [<a href="https://doi.org/10.1007/s11423-019-09681-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Education%20Tech%20Research%20Dev&amp;title=Validating%20theta%20power%20as%20an%20objective%20measure%20of%20cognitive%20load%20in%20educational%20video&amp;author=LJ%20Castro-Meneses&amp;author=J-L%20Kruger&amp;author=S%20Doherty&amp;volume=68&amp;issue=1&amp;publication_year=2020&amp;pages=181-202&amp;doi=10.1007/s11423-019-09681-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref049">
<span class="label">49.</span><cite>Puma S, Matton N, Paubel P-V, Raufaste É, El-Yagoubi R. Using theta and alpha band power to assess cognitive workload in multitasking environments. Int J Psychophysiol. 2018;123:111–20. doi: 10.1016/j.ijpsycho.2017.10.004

</cite> [<a href="https://doi.org/10.1016/j.ijpsycho.2017.10.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29017780/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Psychophysiol&amp;title=Using%20theta%20and%20alpha%20band%20power%20to%20assess%20cognitive%20workload%20in%20multitasking%20environments&amp;author=S%20Puma&amp;author=N%20Matton&amp;author=P-V%20Paubel&amp;author=%C3%89%20Raufaste&amp;author=R%20El-Yagoubi&amp;volume=123&amp;publication_year=2018&amp;pages=111-20&amp;pmid=29017780&amp;doi=10.1016/j.ijpsycho.2017.10.004&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref050">
<span class="label">50.</span><cite>Klimesch W, Schack B, Sauseng P. The functional significance of theta and upper alpha oscillations. Exp Psychol. 2005;52(2):99–108. doi: 10.1027/1618-3169.52.2.99

</cite> [<a href="https://doi.org/10.1027/1618-3169.52.2.99" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15850157/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Exp%20Psychol&amp;title=The%20functional%20significance%20of%20theta%20and%20upper%20alpha%20oscillations&amp;author=W%20Klimesch&amp;author=B%20Schack&amp;author=P%20Sauseng&amp;volume=52&amp;issue=2&amp;publication_year=2005&amp;pages=99-108&amp;pmid=15850157&amp;doi=10.1027/1618-3169.52.2.99&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref051">
<span class="label">51.</span><cite>Boos M, Kobi M, Elmer S, Jäncke L. The influence of experience on cognitive load during simultaneous interpretation. Brain Lang. 2022;234:105185. doi: 10.1016/j.bandl.2022.105185

</cite> [<a href="https://doi.org/10.1016/j.bandl.2022.105185" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36130466/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Lang&amp;title=The%20influence%20of%20experience%20on%20cognitive%20load%20during%20simultaneous%20interpretation&amp;author=M%20Boos&amp;author=M%20Kobi&amp;author=S%20Elmer&amp;author=L%20J%C3%A4ncke&amp;volume=234&amp;publication_year=2022&amp;pages=105185&amp;pmid=36130466&amp;doi=10.1016/j.bandl.2022.105185&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref052">
<span class="label">52.</span><cite>Grabner RH, Brunner C, Leeb R, Neuper C, Pfurtscheller G. Event-related EEG theta and alpha band oscillatory responses during language translation. Brain Res Bull. 2007;72(1):57–65. doi: 10.1016/j.brainresbull.2007.01.001

</cite> [<a href="https://doi.org/10.1016/j.brainresbull.2007.01.001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17303508/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Res%20Bull&amp;title=Event-related%20EEG%20theta%20and%20alpha%20band%20oscillatory%20responses%20during%20language%20translation&amp;author=RH%20Grabner&amp;author=C%20Brunner&amp;author=R%20Leeb&amp;author=C%20Neuper&amp;author=G%20Pfurtscheller&amp;volume=72&amp;issue=1&amp;publication_year=2007&amp;pages=57-65&amp;pmid=17303508&amp;doi=10.1016/j.brainresbull.2007.01.001&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref053">
<span class="label">53.</span><cite>Tran Y, Craig A, Craig R, Chai R, Nguyen H. The influence of mental fatigue on brain activity: Evidence from a systematic review with meta-analyses. Psychophysiology. 2020;57(5):e13554. doi: 10.1111/psyp.13554

</cite> [<a href="https://doi.org/10.1111/psyp.13554" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32108954/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychophysiology&amp;title=The%20influence%20of%20mental%20fatigue%20on%20brain%20activity:%20Evidence%20from%20a%20systematic%20review%20with%20meta-analyses&amp;author=Y%20Tran&amp;author=A%20Craig&amp;author=R%20Craig&amp;author=R%20Chai&amp;author=H%20Nguyen&amp;volume=57&amp;issue=5&amp;publication_year=2020&amp;pmid=32108954&amp;doi=10.1111/psyp.13554&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref054">
<span class="label">54.</span><cite>Isabella SL, Cheyne JA, Cheyne D. Inhibitory Control in the Absence of Awareness: Interactions Between Frontal and Motor Cortex Oscillations Mediate Implicitly Learned Responses. Front Hum Neurosci. 2021;15:786035. doi: 10.3389/fnhum.2021.786035

</cite> [<a href="https://doi.org/10.3389/fnhum.2021.786035" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8727746/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35002659/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Hum%20Neurosci&amp;title=Inhibitory%20Control%20in%20the%20Absence%20of%20Awareness:%20Interactions%20Between%20Frontal%20and%20Motor%20Cortex%20Oscillations%20Mediate%20Implicitly%20Learned%20Responses&amp;author=SL%20Isabella&amp;author=JA%20Cheyne&amp;author=D%20Cheyne&amp;volume=15&amp;publication_year=2021&amp;pages=786035&amp;pmid=35002659&amp;doi=10.3389/fnhum.2021.786035&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref055">
<span class="label">55.</span><cite>Lazarus RS, Folkman S. Stress, appraisal, and coping. Springer Publishing Company;
1984.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Stress,%20appraisal,%20and%20coping&amp;author=RS%20Lazarus&amp;author=S%20Folkman&amp;publication_year=1984&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref056">
<span class="label">56.</span><cite>Kurz I. A look into the ‘black box’–EEG probability mapping during mental simultaneous interpreting. Translation Stud. 1994;2:199.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Translation%20Stud&amp;title=A%20look%20into%20the%20%E2%80%98black%20box%E2%80%99%E2%80%93EEG%20probability%20mapping%20during%20mental%20simultaneous%20interpreting&amp;author=I%20Kurz&amp;volume=2&amp;publication_year=1994&amp;pages=199&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref057">
<span class="label">57.</span><cite>Kurz I. Watching the brain at work-an exploratory study of EEG changes during Simultaneous Interpreting (SI). 1995.</cite>
</li>
<li id="pone.0330692.ref058">
<span class="label">58.</span><cite>Koshkin R, Shtyrov Y, Myachykov A, Ossadtchi A. Testing the efforts model of simultaneous interpreting: An ERP study. PLoS One. 2018;13(10):e0206129. doi: 10.1371/journal.pone.0206129

</cite> [<a href="https://doi.org/10.1371/journal.pone.0206129" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6200263/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30356337/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Testing%20the%20efforts%20model%20of%20simultaneous%20interpreting:%20An%20ERP%20study&amp;author=R%20Koshkin&amp;author=Y%20Shtyrov&amp;author=A%20Myachykov&amp;author=A%20Ossadtchi&amp;volume=13&amp;issue=10&amp;publication_year=2018&amp;pmid=30356337&amp;doi=10.1371/journal.pone.0206129&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref059">
<span class="label">59.</span><cite>Moser-Mercer B. Simultaneous interpreting: Cognitive potential and limitations. Interpreting. 2000;5(2):83–94.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Interpreting&amp;title=Simultaneous%20interpreting:%20Cognitive%20potential%20and%20limitations&amp;author=B%20Moser-Mercer&amp;volume=5&amp;issue=2&amp;publication_year=2000&amp;pages=83-94&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref060">
<span class="label">60.</span><cite>Yagura H, Tanaka H, Kinoshita T, Watanabe H, Motomura S, Sudoh K, et al. Selective Attention Measurement of Experienced Simultaneous Interpreters Using EEG Phase-Locked Response. Front Hum Neurosci. 2021;15:581525. doi: 10.3389/fnhum.2021.581525

</cite> [<a href="https://doi.org/10.3389/fnhum.2021.581525" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8215497/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34163336/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Hum%20Neurosci&amp;title=Selective%20Attention%20Measurement%20of%20Experienced%20Simultaneous%20Interpreters%20Using%20EEG%20Phase-Locked%20Response&amp;author=H%20Yagura&amp;author=H%20Tanaka&amp;author=T%20Kinoshita&amp;author=H%20Watanabe&amp;author=S%20Motomura&amp;volume=15&amp;publication_year=2021&amp;pages=581525&amp;pmid=34163336&amp;doi=10.3389/fnhum.2021.581525&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref061">
<span class="label">61.</span><cite>Delorme A, Makeig S. EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis. J Neurosci Methods. 2004;134(1):9–21. doi: 10.1016/j.jneumeth.2003.10.009

</cite> [<a href="https://doi.org/10.1016/j.jneumeth.2003.10.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15102499/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Neurosci%20Methods&amp;title=EEGLAB:%20an%20open%20source%20toolbox%20for%20analysis%20of%20single-trial%20EEG%20dynamics%20including%20independent%20component%20analysis&amp;author=A%20Delorme&amp;author=S%20Makeig&amp;volume=134&amp;issue=1&amp;publication_year=2004&amp;pages=9-21&amp;pmid=15102499&amp;doi=10.1016/j.jneumeth.2003.10.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref062">
<span class="label">62.</span><cite>Palva S, Palva JM. New vistas for alpha-frequency band oscillations. Trends Neurosci. 2007;30(4):150–8. doi: 10.1016/j.tins.2007.02.001

</cite> [<a href="https://doi.org/10.1016/j.tins.2007.02.001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17307258/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Neurosci&amp;title=New%20vistas%20for%20alpha-frequency%20band%20oscillations&amp;author=S%20Palva&amp;author=JM%20Palva&amp;volume=30&amp;issue=4&amp;publication_year=2007&amp;pages=150-8&amp;pmid=17307258&amp;doi=10.1016/j.tins.2007.02.001&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref063">
<span class="label">63.</span><cite>Liao S, Yu L, Kruger J-L, Reichle ED. The impact of audio on the reading of intralingual versus interlingual subtitles: Evidence from eye movements. Appl Psychol. 2021;43(1):237–69. doi: 10.1017/s0142716421000527</cite> [<a href="https://doi.org/10.1017/s0142716421000527" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Psychol&amp;title=The%20impact%20of%20audio%20on%20the%20reading%20of%20intralingual%20versus%20interlingual%20subtitles:%20Evidence%20from%20eye%20movements&amp;author=S%20Liao&amp;author=L%20Yu&amp;author=J-L%20Kruger&amp;author=ED%20Reichle&amp;volume=43&amp;issue=1&amp;publication_year=2021&amp;pages=237-69&amp;doi=10.1017/s0142716421000527&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref064">
<span class="label">64.</span><cite>Gerber-Morón O, Szarkowska A, Woll B. The impact of text segmentation on subtitle reading. J Eye Mov Res. 2018;11(4):10.16910/jemr.11.4.2. doi: 10.16910/jemr.11.4.2

</cite> [<a href="https://doi.org/10.16910/jemr.11.4.2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7901653/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33828704/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Eye%20Mov%20Res&amp;title=The%20impact%20of%20text%20segmentation%20on%20subtitle%20reading&amp;author=O%20Gerber-Mor%C3%B3n&amp;author=A%20Szarkowska&amp;author=B%20Woll&amp;volume=11&amp;issue=4&amp;publication_year=2018&amp;pmid=33828704&amp;doi=10.16910/jemr.11.4.2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref065">
<span class="label">65.</span><cite>Perego E, Del Missier F, Porta M, Mosconi M. The Cognitive Effectiveness of Subtitle Processing. Media Psychol. 2010;13(3):243–72. doi: 10.1080/15213269.2010.502873</cite> [<a href="https://doi.org/10.1080/15213269.2010.502873" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Media%20Psychol&amp;title=The%20Cognitive%20Effectiveness%20of%20Subtitle%20Processing&amp;author=E%20Perego&amp;author=F%20Del%20Missier&amp;author=M%20Porta&amp;author=M%20Mosconi&amp;volume=13&amp;issue=3&amp;publication_year=2010&amp;pages=243-72&amp;doi=10.1080/15213269.2010.502873&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref066">
<span class="label">66.</span><cite>Mayer RE, Moreno R. Nine Ways to Reduce Cognitive Load in Multimedia Learning. Education Psychol. 2003;38(1):43–52. doi: 10.1207/s15326985ep3801_6</cite> [<a href="https://doi.org/10.1207/s15326985ep3801_6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Education%20Psychol&amp;title=Nine%20Ways%20to%20Reduce%20Cognitive%20Load%20in%20Multimedia%20Learning&amp;author=RE%20Mayer&amp;author=R%20Moreno&amp;volume=38&amp;issue=1&amp;publication_year=2003&amp;pages=43-52&amp;doi=10.1207/s15326985ep3801_6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref067">
<span class="label">67.</span><cite>Mayer RE. Cognitive Theory of Multimedia Learning. Cambridge Handbook Multimedia Learn. 2005;41(1):31–48.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Cambridge%20Handbook%20Multimedia%20Learn&amp;title=Cognitive%20Theory%20of%20Multimedia%20Learning&amp;author=RE%20Mayer&amp;volume=41&amp;issue=1&amp;publication_year=2005&amp;pages=31-48&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref068">
<span class="label">68.</span><cite>Paivio A, Desrochers A. A dual-coding approach to bilingual memory. Canadian J Psychol/ Revue canadienne de psychologie. 1980;34(4):388–99. doi: 10.1037/h0081101</cite> [<a href="https://doi.org/10.1037/h0081101" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Canadian%20J%20Psychol/%20Revue%20canadienne%20de%20psychologie&amp;title=A%20dual-coding%20approach%20to%20bilingual%20memory&amp;author=A%20Paivio&amp;author=A%20Desrochers&amp;volume=34&amp;issue=4&amp;publication_year=1980&amp;pages=388-99&amp;doi=10.1037/h0081101&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref069">
<span class="label">69.</span><cite>Schnotz W. An integrated model of text and picture comprehension. In: The Cambridge handbook of multimedia learning. West Nyack: Cambridge University Press;
2005.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20Cambridge%20handbook%20of%20multimedia%20learning&amp;author=W%20Schnotz&amp;publication_year=2005&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref070">
<span class="label">70.</span><cite>Justice LM, Lomax R, O’Connell A, Pentimonti J, Petrill SA, Piasta SB, et al. Are working memory and behavioral attention equally important for both reading and listening comprehension? A developmental comparison. Read Writing. 2018;31(7):1449–77.</cite> [<a href="https://doi.org/10.1007/s11145-018-9840-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6096896/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30147241/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Read%20Writing&amp;title=Are%20working%20memory%20and%20behavioral%20attention%20equally%20important%20for%20both%20reading%20and%20listening%20comprehension?%20A%20developmental%20comparison&amp;author=LM%20Justice&amp;author=R%20Lomax&amp;author=A%20O%E2%80%99Connell&amp;author=J%20Pentimonti&amp;author=SA%20Petrill&amp;volume=31&amp;issue=7&amp;publication_year=2018&amp;pages=1449-77&amp;pmid=30147241&amp;doi=10.1007/s11145-018-9840-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref071">
<span class="label">71.</span><cite>Mayer RE. The Cambridge Handbook of Multimedia Learning. 2nd ed. Cambridge Handbooks in Psychology. West Nyack: Cambridge University Press;
2014.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20Cambridge%20Handbook%20of%20Multimedia%20Learning&amp;author=RE%20Mayer&amp;publication_year=2014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref072">
<span class="label">72.</span><cite>Nishida N, Rekimoto J. SUMART: SUMmARizing Translation from Wordy to Concise Expression. In: 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW). IEEE;
2024. p. 613–5.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2024%20IEEE%20Conference%20on%20Virtual%20Reality%20and%203D%20User%20Interfaces%20Abstracts%20and%20Workshops%20(VRW)&amp;author=N%20Nishida&amp;author=J%20Rekimoto&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref073">
<span class="label">73.</span><cite>Sun G. ClinClip: a Multimodal Language Pre-training model integrating EEG data for enhanced English medical listening assessment. Front Neurosci. 2025;18:1493163. doi: 10.3389/fnins.2024.1493163

</cite> [<a href="https://doi.org/10.3389/fnins.2024.1493163" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11755411/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39850622/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Neurosci&amp;title=ClinClip:%20a%20Multimodal%20Language%20Pre-training%20model%20integrating%20EEG%20data%20for%20enhanced%20English%20medical%20listening%20assessment&amp;author=G%20Sun&amp;volume=18&amp;publication_year=2025&amp;pages=1493163&amp;pmid=39850622&amp;doi=10.3389/fnins.2024.1493163&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref074">
<span class="label">74.</span><cite>Chen Y, Wang W, Yan S, Wang Y, Zheng X, Lv C. Application of electroencephalography sensors and artificial intelligence in automated language teaching. Sensors. 2024;24(21):6969.
</cite> [<a href="https://doi.org/10.3390/s24216969" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11548684/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39517865/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sensors&amp;title=Application%20of%20electroencephalography%20sensors%20and%20artificial%20intelligence%20in%20automated%20language%20teaching&amp;author=Y%20Chen&amp;author=W%20Wang&amp;author=S%20Yan&amp;author=Y%20Wang&amp;author=X%20Zheng&amp;volume=24&amp;issue=21&amp;publication_year=2024&amp;pages=6969&amp;pmid=39517865&amp;doi=10.3390/s24216969&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330692.ref075">
<span class="label">75.</span><cite>Huang Y, Li D, Cheung AKF. Evaluating the linguistic complexity of machine translation and LLMs for EFL/ESL applications: An entropy weight method. Res Method Appl Linguist. 2025;4(3):100229. doi: 10.1016/j.rmal.2025.100229</cite> [<a href="https://doi.org/10.1016/j.rmal.2025.100229" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Res%20Method%20Appl%20Linguist&amp;title=Evaluating%20the%20linguistic%20complexity%20of%20machine%20translation%20and%20LLMs%20for%20EFL/ESL%20applications:%20An%20entropy%20weight%20method&amp;author=Y%20Huang&amp;author=D%20Li&amp;author=AKF%20Cheung&amp;volume=4&amp;issue=3&amp;publication_year=2025&amp;pages=100229&amp;doi=10.1016/j.rmal.2025.100229&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section></section><article class="sub-article" id="pone.0330692.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330692.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330692.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Orrego-Carmona%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">David Orrego-Carmona</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">David Orrego-Carmona</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Orrego-Carmona%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">David Orrego-Carmona</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">David Orrego-Carmona</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 David Orrego-Carmona</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>7 Apr 2025</em>
</p>
<p>PONE-D-25-04673Simultaneous interpreting with auto-subtitling: investigating viewers’ cognitive effort, stress, and comprehensionPLOS ONE</p>
<p>Dear Dr. Cheung,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.Both reviewers have provided detailed and actionable feedback. Please read their recommendations carefully and make sure you address their points. Based on their comments, it would also be useful to thoroughly edit and proof-read the manuscript.</p>
<p>Please submit your revised manuscript by May 22 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>David Orrego-Carmona, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>When submitting your revision, we need you to address these additional requirements.</p>
<p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at <a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and <a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a></p>
<p>2. Thank you for stating the following financial disclosure:</p>
<p>We would like to thank the Research and Innovation Office of the Hong Kong Polytechnic University for supporting the project (Project Code: RKQY). This manuscript was also partly supported by funding from Project number P0046386 of the Department of Chinese and Bilingual Studies of the Hong Kong Polytechnic University.</p>
<p>Please state what role the funders took in the study. If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p>
<p>If this statement is not correct you must amend it as needed.</p>
<p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p>
<p>3. Please include your full ethics statement in the ‘Methods’ section of your manuscript file. In your statement, please include the full name of the IRB or ethics committee who approved or waived your study, as well as whether or not you obtained informed written or verbal consent. If consent was waived for your study, please include this information in your statement as well.</p>
<p>4. We note that you have indicated that there are restrictions to data sharing for this study. For studies involving human research participant data or other sensitive data, we encourage authors to share de-identified or anonymized data. However, when data cannot be publicly shared for ethical reasons, we allow authors to make their data sets available upon request. For information on unacceptable data access restrictions, please see <a href="http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions</a>.</p>
<p>Before we proceed with your manuscript, please address the following prompts:</p>
<p>a) If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially identifying or sensitive patient information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., a Research Ethics Committee or Institutional Review Board, etc.). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent.</p>
<p>b) If there are no restrictions, please upload the minimal anonymized data set necessary to replicate your study findings to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. Please see <a href="http://www.bmj.com/content/340/bmj.c181.long" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.bmj.com/content/340/bmj.c181.long</a> for guidelines on how to de-identify and prepare clinical data for publication. For a list of recommended repositories, please see <a href="https://journals.plos.org/plosone/s/recommended-repositories" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/recommended-repositories</a>. You also have the option of uploading the data as Supporting Information files, but we would recommend depositing data directly to a data repository if possible.</p>
<p>Please update your Data Availability statement in the submission form accordingly.</p>
<p>5. In the online submission form, you indicated that the data underlying the results presented in the study are available from the authors upon request.</p>
<p>All PLOS journals now require all data underlying the findings described in their manuscript to be freely available to other researchers, either 1. In a public repository, 2. Within the manuscript itself, or 3. Uploaded as supplementary information.</p>
<p>This policy applies to all data except where public deposition would breach compliance with the protocol approved by your research ethics board. If your data cannot be made publicly available for ethical or legal reasons (e.g., public availability would compromise patient privacy), please explain your reasons on resubmission and your exemption request will be escalated for approval.</p>
<p>6. Please remove all personal information, ensure that the data shared are in accordance with participant consent, and re-upload a fully anonymized data set.</p>
<p>Note: spreadsheet columns with personal information must be removed and not hidden as all hidden columns will appear in the published file.</p>
<p>Additional guidance on preparing raw data for publication can be found in our Data Policy (<a href="https://journals.plos.org/plosone/s/data-availability#loc-human-research-participant-data-and-other-sensitive-data" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/data-availability#loc-human-research-participant-data-and-other-sensitive-data</a>) and in the following article: <a href="http://www.bmj.com/content/340/bmj.c181.long" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.bmj.com/content/340/bmj.c181.long</a>.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Partly</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>5. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #1: I appreciate the opportunity to review this manuscript. This study offers a timely and methodologically innovative exploration of how automatic subtitling combined with simultaneous interpreting affects cognitive effort, stress, and comprehension in conference settings. By integrating EEG to objectively measure cognitive load alongside comprehension tests, the authors provide robust empirical insights into multimodal information processing. The key finding is that dual-modality delivery reduces cognitive demands without compromising comprehension. This has direct implications for designing accessible multilingual communication systems, particularly in virtual or hybrid conferences. The use of neuroscientific tools to validate human-AI collaboration in interpreting contexts represents a compelling advancement for audiovisual interpreting studies and cognitive science, bridging technology with human-centered outcomes.</p>
<p>While the manuscript is well-written and contributes to the field, I recommend a few minor adjustments to enhance its overall impact:</p>
<p>- The interchangeable use of “auto-subtitles”, “automatic subtitles” and “AI-generated subtitles” creates ambiguity. Choose one and use it consistently throughout the paper to avoid confusion.</p>
<p>- Integrate 2023–2024 studies on AI-driven subtitling (e.g., large language models in real-time transcription) and neurocognitive mechanisms of multimodal processing (e.g., theta-beta coupling in cognitive load).</p>
<p>- It would be valuable for the authors to suggest interesting future research directions based on the findings of this study.</p>
<p>- In the practical implications, discussing how the insights from this study could influence real-world applications, particularly in relation to the viewer’s cognitive well-being, would further enhance the impact of this work.</p>
<p>- It would be valuable for the authors to suggest interesting future research directions based on the findings of this study.</p>
<p>- The resolution of the figures in this manuscript should be improved.</p>
<p>This study makes a compelling case for integrating automatic subtitling with interpreting in multilingual conferences. Its innovative methodology, theoretical contributions, and practical relevance align strongly with the journal’s interdisciplinary focus on technology-mediated communication. While minor revisions are needed to enhance clarity and depth, the core findings are significant and merit publication.</p>
<p>Reviewer #2: Your article addresses pertinent issues and presents valuable ideas. The content is engaging and has the potential to contribute to the field. However, I have some suggestions that could enhance its clarity and depth.</p>
<p>As noted in the commented file, several core concepts are not always clearly contextualised. It would be beneficial to provide clearer definitions/descriptions and background information for these key terms.</p>
<p>While your literature review is a good start, I recommend incorporating more supporting sources throughout the article. Integrating additional studies and references beyond the initial literature review can strengthen your claims and provide a broader context for your findings. The case for the research gap is not convincing enough.</p>
<p>I suggest revisiting your research questions (RQs). This could also clarify the scope of your research.</p>
<p>The final sections of the article could be more concise. Streamlining these parts will help in emphasisiing the key takeaways.</p>
<p>In several instances, the manuscript could benefit from additional details, particularly in the methodology and results sections. Supplying more information regarding your methods, analyses, and the context of your results will enhance the understanding and robustness of your work.</p>
<p>Lastly, using clearer language throughout the article will improve readability. The main thing here might just be avoiding the perception of repetition.</p>
<p>**********</p>
<p>6. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a> .</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span> . Please note that Supporting Information files do not need this step.</p>
<section class="sm xbox font-sm" id="pone.0330692.s004"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>PONE-D-25-04673_reviewer.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s004.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s004.pdf</a><sup> (1.7MB, pdf) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330692.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 22;20(8):e0330692. doi: <a href="https://doi.org/10.1371/journal.pone.0330692.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330692.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>21 May 2025</em>
</p>
<p>Please refer to the uploaded file named "response to reviewers".</p>
<section class="sm xbox font-sm" id="pone.0330692.s006"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s006.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s006.docx</a><sup> (56.4KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330692.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330692.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330692.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Berghoff%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Robyn Berghoff</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Robyn Berghoff</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Berghoff%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Robyn Berghoff</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Robyn Berghoff</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Robyn Berghoff</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>5 Aug 2025</em>
</p>
<p>Simultaneous interpreting with auto-subtitling: investigating viewers’ cognitive effort, stress, and comprehension</p>
<p>PONE-D-25-04673R1</p>
<p>Dear Dr. Cheung,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a>  and clicking the ‘Update My Information' link at the top of the page. For questions related to billing, please contact <a href="https://plos.my.site.com/s/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">billing support</a> .</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>Kind regards,</p>
<p>Robyn Berghoff, PhD</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Please note you may disregard Reviewer 3's additional comments re: issues they would like to see addressed before publication. </p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
<p>Reviewer #1: All comments have been addressed</p>
<p>Reviewer #3: All comments have been addressed</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #3: No</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>6. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #1: I thank the authors for their revisions. I have no further comments and would be happy to see the paper published.</p>
<p>Reviewer #3: Thank you very much for uploading your manuscript. I have thoroughly studied your paper and other reviewer's opinions and your answers. It deems necessary to appreciate you to do such an invaluable research. There are some issues that I suppose they should be addressed before publishing.</p>
<p>First of all, on page 9 of the paper, the color of the waves and cognitive cases are similar to some degree. Please change them. I have seen in the following parts that you have used more colorful pictures; therefore, this is of no hardship for you.</p>
<p>Secondly, you have chosen Arabic as the preferred language. Why have you not selected other languages and do you and your team have proficiency in this language? Since even the interpreted versions may have some problems. How would have you understood the discrepancies with the correct version? Have you cooperated with any Arabic to Chinese interpreter/translator in this regard?</p>
<p>Thirdly, in the experimental section, you have mentioned that you have prepared subjective questionnaire; however, you have only mentioned one of the questions that you have change the wording. I couldn't find the questionnaire at the end of your paper! Please include it with all the responses.</p>
<p>Last but not least s the case of adding ASR in your keywords since this phrase has been frequently used in your paper.</p>
<p>**********</p>
<p>7. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a> .</p>
<p>Reviewer #1: No</p>
<p>Reviewer #3: <strong>Yes: </strong> Neda Patdad</p>
<p>**********</p></section></article><article class="sub-article" id="pone.0330692.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330692.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330692.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Berghoff%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Robyn Berghoff</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Robyn Berghoff</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Berghoff%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Robyn Berghoff</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.f" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.f" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Robyn Berghoff</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.f" class="d-panel p" style="display: none">
<div>© 2025 Robyn Berghoff</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-25-04673R1</p>
<p>PLOS ONE</p>
<p>Dear Dr. Cheung,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at customercare@plos.org.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Dr. Robyn Berghoff</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>S1 File. Questionnaire in EN.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s001.docx</a><sup> (11.9KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>S2 File. EEG based data.</span><p>(XLSX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s002.xlsx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s002.xlsx</a><sup> (44.3KB, xlsx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="caption p">
<span>S3 File. Comprehension data.</span><p>(XLSX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s003.xlsx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s003.xlsx</a><sup> (10.2KB, xlsx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material4_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>PONE-D-25-04673_reviewer.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s004.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s004.pdf</a><sup> (1.7MB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material5_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373249/bin/pone.0330692.s006.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330692.s006.docx</a><sup> (56.4KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>All relevant data are within the manuscript and its <a href="#sec020" class="usa-link">Supporting information</a> files.</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0330692"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0330692.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (1.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373249/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373249/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373249%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373249/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373249/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373249/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40845000/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373249/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40845000/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373249/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373249/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="JlgCfNVz9zdXE79rLuhgdvQTCwZWyFxe29jFRDDyqyttZiX3dNrKHV9UQxwJp6d8">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
