This project aims to examine the effect of pediatric virtual patients (VPs) on audiology students' perception of self‐efficacy and characterize student reflections of using pediatric VPs.
A mixed methods convergent parallel design was employed for this study.
To measure the effect of VPs on self‐efficacy the learning self‐efficacy scale (L‐SES) for clinical skills was used.
A thematic analysis of open‐ended survey questions was completed to establish student reflections.
A total of 92 first‐year Master of Clinical Audiology students from two cohorts participated, with 38 completing both pre‐ and post‐VP L‐SES.
Statistically significant changes in self‐efficacy were seen for cognitive and psychomotor domains.
Five themes were established: Gaining experience, feedback for learning, time and place, usability considerations and recommendations for future applications.
This study indicates that the use of pediatric behavioral hearing assessment VPs can drive gains in self‐reported self‐efficacy in cognitive and psychomotor domains.
Gains to affective self‐efficacy improvements are likely harmed by usability issues.
These findings add to a growing evidence base on the educational value of VPs as it relates to self‐efficacy within audiology and the wider health care field.
Carefully designed and implemented virtual patients can drive self‐efficacy in cognitive and psychomotor domains, while usability concerns could harm self‐efficacy in the affective domain.
Students value the engaging and experiential nature of pediatric VPs, associated feedback provision, and support their place in the curriculum.
Students desire more complex and non‐technical skills‐focused VPs to be implemented going forward.
Virtual patients (VP) allow students to interact with a digital representation of a patient in a self‐directed manner and appear to be an effective way of improving clinical knowledge and skills (Kononowicz et al.2019).
Use of the term “Virtual Patients” originated in the early 1990s in reference to software that allows students to simulate patient care and develop a range of clinical skills (Kononowicz et al.2015).
VPs have been used in medicine, nursing, and pharmacy, with many studies reporting general positivity and greater engagement of students using VPs (compared to traditional classroom‐based teaching) (Kononowicz et al.2019).
As VPs are used by adult learners in healthcare training programs, consulting Andragogy theory can help to understand how self‐directed learning, prior experience, and student beliefs around application of knowledge to real life may impact learning outcomes (Knowles et al.2020).
It remains unclear whether skills acquired from this kind of simulation activity transfer to demonstrable real‐life skill (Cook et al.2010).
Greater evidence is also needed to demonstrate whether VPs drive self‐confidence in clinical abilities and how this relates to clinical performance across allied health professions, given most literature focuses on their application for medical training (Kononowicz et al.2019).
Self‐efficacy, or confidence, is one of the underlying principles of clinical education and suggests that an individual's self‐belief in their ability to successfully perform certain actions is directly related to their confidence in these actions (Bandura1977).
Self‐efficacy has been shown to be strongly associated with (and a reliable predictor of) behavior change and student achievement (Bartimote‐Aufflick et al.2016).
Studies from nursing have provided examples of significant improvements in self‐efficacy following completion of digital simulations (Cardoza and Hood2012; Chang et al.2022; Mabry et al.2020).
While parallels can often be drawn between health professions, more evidence from a range of disciplines outside of medicine and nursing is needed.
Audiology is a discipline that requires unique clinical skills, from hearing aid fittings to diagnostic testing across a range of sub‐disciplines, including vestibular and pediatric domains.
There is a general paucity of evidence into educational outcomes of Audiology students using digital simulations amidst calls for increased use of simulation in Audiology training programs and therefore a need for a greater evidence base on their use (Alanazi and Nicholson2023).
Research to date into digital simulation use in Audiology specifically has demonstrated that e‐learning innovations can lead to knowledge gains similar to other traditional teaching methods and be positively received by students (Alanazi and Nicholson2023; Tomlin et al.2023).
Despite this, there remains a significant gap in research regarding the application and effectiveness of VPs in Audiology training programs.
Given their applicability for developing skills that may be difficult to practice through other standard learning activities, further investigation of their use in this specific context is needed.
Guided by Bandura's self‐efficacy theory and Knowles' Andragogy theory, this project aims to determine if a suite of pediatric VPs is shown to positively impact students' self‐perceived efficacy of clinical skill development and to characterize student reflections of using pediatric VPs for learning how to perform behavioral hearing assessments.
This study employed a mixed methods convergent parallel design, combining a repeated measures Likert scale survey to investigate the impact of using pediatric VPs on Audiology students' self‐efficacy and a qualitative cross‐sectional survey to explore the experiences of students using pediatric VPs.
It carried ethics approval from University of Melbourne Human Research Ethics Committee (project 24031) and occurred between June 2022 and October 2023.
The Master of Clinical Audiology is a 2‐year entry to practice Audiology program, where students achieve competency in key clinical domains.
The pediatric VPs described in this study were integrated into the second semester, first year curriculum.
In this semester, students have learned the basics of pediatric behavioral testing techniques but are yet to consolidate or advance these skills.
The VPs aim to provide an opportunity for students to further develop their pediatric assessment skills within workshops (and outside of a clinical setting).
This study employed a convenience sampling strategy, inviting all 138 students from two consecutive cohorts (62 and 76, respectively) of first‐year Master of Clinical Audiology students to participate.
Participants were invited by announcements over their learning management software (LMS), which included a link to a plain language statement further explaining details of the project.
Informed consent was obtained explicitly from each participant by participants providing written consent in an online form preceding the survey.
Participation was voluntary, did not involve incentives, and did not alter planned student learning activities in any way during the study period.
No additional information about participants was sought outside of their student ID to match their responses before and after use of VPs and to a measure of their baseline skill level.
Six VPs were used by students as part of their coursework.
The hearing test techniques, along with the learning objectives of each, are reported in Figure1.
Hearing test techniques, names, and learning objectives of each of the virtual patients, presented in order of use by students.
The VP learning experience was guided by both Bandura's self‐efficacy theory and Knowles' Andragogy theory (Bandura1977; Knowles et al.2020).
With self‐efficacy theory in mind, we designed the VPs to offer students an experiential learning opportunity, where they could actively demonstrate performance accomplishments, receive verbal persuasion (encouraging feedback), and be engaged with the patient cases.
Considering Andragogy theory, we ensured it was clear to students why we were presenting this learning task, highlighting the subject learning objectives and the need for students to gain competency in pediatric behavioral hearing assessments to become proficient Audiology graduates.
The pediatric VPs were all designed by a team composed of a subject matter expert (academic Pediatric Audiologist) and a learning designer.
Articulate Storyline and Adobe Photoshop were used to create the VPs.
Gamification principles were used to replicate a true clinical experience.
Specifically, a complete narrative for the student Audiologist and patient was presented, and a range of visual and interactive features were accessible, such as animated characters, clickable buttons, dials, and text entry boxes to mimic real clinical encounters and processes.
Furthermore, automated formative feedback on performance was provided to students as they progressed through the VP case.
This occurred in both a visual format to represent timing pressures, stimuli presentations, and child reactions (Figure2), as well as in text format (Figure3).
Screenshot of VP “Sophie: Visual Reinforcement Audiometry,” showing visual feedback in the form of a timing feature and child reactions to stimuli.
Screenshot of VP “Amy: Kendall Toy Test,” showing text based formative feedback.
Both cohorts received the same VP curriculum.
Within classes across the semester, students were directed to access the VPs through the LMS.
A pre‐brief involved orienting students to the task by discussing which behavioral assessment was the focus of the VP and key learning objectives.
In addition to this pre‐briefing was a within VP pre‐brief, which gave additional details about the VP biography, aims, and how to navigate controls.
The students then completed the VPs on their own brought‐from‐home devices.
A short debrief in the form of revisiting learning objectives was presented within VPs, and a whole‐class debrief involving a reflective analysis followed.
Following class, the VPs remained available on the LMS for students to revisit as they pleased.
The learning self‐efficacy scale (L‐SES) for clinical skills was used (with permission) as the main outcome measure (Kang et al.2019).
The L‐SES survey consists of 12 items, with four items in each of the following three domains: Cognitive, affective, and psychomotor.
The cognitive domain assesses students' confidence in their knowledge levels, while the affective domain measures students' emotions and feelings around their development.
Finally, the psychomotor domain is a measure of self‐efficacy in relation to physical skill implementation.
The final version developed and validated by Kang et al. (2019) was based on Bloom's taxonomy framework and achieved a Cronbach'sαcoefficient of 0.931, showing high reliability.
Cronbach'sαfor the exact L‐SES applied in this study (AppendixS1A) was 0.85, indicating a high level of internal consistency.
This 5‐point Likert scale tool (1 =strongly disagreeto 5 =strongly agree) was applied twice—prior to completing the first VP at the start of the semester and after completion of the final VP at the end of the semester.
Scores were summed across each domain and for the total 12 items (with possible mean scores across the cohort of 4–20 within domains and 12–60 for all items combined).
A survey with open‐ended questions was administered to students following the use of six VPs (AppendixS1B).
The survey remained open for 2 weeks following the use of the final VP, and following this, responses were imported into NVivo Windows (release 1) software.
All surveys were hosted online, using Qualtrics software (Qualtrics, Provo, UT, USA).
Likert scale data was analyzed using Minitab version 21.3, and power analysis was completed in G*Power.
Baseline skill level scores (out of 100) from a pediatric Objective Structured Clinical Examination (OCSE) performed at the end of the semester prior to using the VPs were analyzed using at‐test to compare cohorts and determine the appropriateness of combining them.
Likert Scale scores were summed for all survey items (as well as within each domain) and analyzed using mixed effects ANOVA to compare ratings within individuals pre‐ and post‐VP use.
A post hoc power analysis for repeated measures within factors ANOVA was performed in G*Power.
For this analysis, we usedα= 0.05 (two‐tailed), effect sizef= 0.25, one group, sample size of 38, and two measurements.
This revealed a power of 0.85.
Statistical significance was set atp< 0.05 for all analyses.
A reflexive thematic analysis of the survey data occurred, as described by Braun and Clarke (2021). This was composed of six phases: familiarization, coding, generating themes, reviewing and developing themes, defining themes, and reporting the analysis.
An inductive approach to coding and thematic generation was employed, whereby the content of student responses informed the result.
While reviewing survey responses, the first author familiarized themselves with and generated initial notes about the data.
They then initiated code generation and proceeded to code refinement and initial theme formation in consultation with Author 2.
This resulted in 146 codes and five themes being generated.
Both authors discussed the content of each theme in an iterative process of refinement until consensus over each one was reached.
Once themes were defined and named, Author 1 wrote up the final report.
Through peer debriefing and reflexive discussions, the authors aimed to acknowledge and mitigate any preconceived assumptions that could affect data analysis, given both are Audiology teaching academics.
Furthermore, Likert and open‐ended survey data were triangulated to improve the credibility of the findings, with quantitative results contextualizing and validating the qualitative findings.
A total of 92 students completed the full pre‐VP L‐SES (response rate of 67%), while 38 completed the full post‐VP L‐SES (response rate 28%).
All results are based on the cohort of students who completed both pre‐ and post‐VP L‐SES in full.
An independentt‐test was performed to establish if there were any significant differences in baseline skill levels between the different year cohorts, finding no significant difference,t(36) = 0.60,p= 0.55.
Effect size was small (Cohen'sd= 0.20), showing a minimal practical difference between cohorts.
Mean cohort skill scores for each of the two groups equaled 89.8 (SD = 9.4) and 87.8 (SD = 10.4), respectively.
Given each cohort of students was independent of one another and the difference in baseline performance was insignificant, they were combined for further analyses.
The means of summed Likert scale scores for each of the L‐SES domains (and overall) for all 38 students can be found in Figure4.
To investigate self‐efficacy score changes before and after VP use, a mixed effects ANOVA was performed.
Our dependent variable was the summed scores of each domain and all domains combined, with a fixed effect of time (pre vs. post‐VP use) and a covariate of their baseline skill score.
Student ID was used as a random effect to account for individual differences.
This model revealed significant improvements in self‐efficacy over time in the cognitive domain,F(1, 37) = 9.10,p= 0.005, and the psychomotor domain,F(1, 37) = 9.74,p= 0.003.
Significant improvements were not established for the affective domain,F(1, 37) = 2.71,p= 0.108, nor all domains combined,F(1, 37) = 0.88,p= 0.355.
Interval plots depicting the 38 students' mean summed scores pre‐ and post‐VP for (A).
Each domain and (B).
All domains combined (error bars = 95% CI for the mean).
Mixed effects ANOVA showed significant changes for cognitive and psychomotor domains (with significance reported as *p< 0.01).
Thematic analysis of 45 student responses to open‐ended survey questions led to the following five themes being formed: Gaining experience, Feedback for learning, Time and place, Usability considerations, and Recommendations for future applications.
Overall, feedback was viewed as useful for learning, and its delivery can take on many formats, with mixed views as to the best way to deliver this feedback.
Despite these factors affording good usability, criticisms were made of certain aspects, with students experiencing different levels of difficulty in using the VPs.
The timing element in the VRA VP that aimed to teach testing efficiency was described as overwhelming for some students.
Some students said they were too short and could be more challenging; other expressed frustrations with their length and responsiveness.
In this study, we established that while students' self‐efficacy increased in some domains following the use of a suite of pediatric behavioral assessment VPs, this effect was not seen across all measures.
Despite this, gains in self‐efficacy in the cognitive domain indicate that students felt more knowledgeable after completion of the pediatric VPs.
For the psychomotor domain, results indicated that students gained greater self‐belief in their ability.
These audiology‐based findings align with studies from nursing, which have demonstrated gains in self‐efficacy following the use of digital simulations (Cardoza and Hood2012; Chang et al.2022; Mabry et al.2020).
Themes 1–3 may also provide more insight into the drivers of positive change in these self‐efficacy domains, whereby students reported feeling as though they gained experience in the procedural aspects of pediatric hearing testing and were able to gain insights into processes that they demonstrated knowledge of (or lack thereof).
In Theme 1, students reported a high degree of engagement with the VPs, reflecting literature on VPs in healthcare education (Cook et al.2010).
Students highlighted through Theme 2 how provision of feedback that was clear, immediate, and formative was impactful for their learning.
This finding aligns with other health professions literature, which describes feedback that is timely, effective, and credible as crucial for learning (Poulos and Mahony2008).
Our findings are also consistent with those of Cook et al. (2010) and Dahri et al. (2019), both reporting how students believe the provision of detailed, formative feedback enhances the VP learning experience.
In terms of curricular integration, Theme 3 explored how students felt that using VPs to consolidate knowledge and prepare for clinical placements or exams was the ideal way to utilize them.
Introducing VPs at timepoints with relevance to other course material has previously been reported as preferred by students (Dahri et al.2019).
However, when it comes to the physical location that students use VPs, students suggested in our study that using VPs at home may be as beneficial as using them in classes or small groups.
This has not been reported in previous studies (perhaps as mostly VPs are typically only available to students within class) and is a finding that warrants further investigation.
Despite the gains over time seen within individuals for cognitive and psychomotor domains, we saw no significant improvement in self‐efficacy for the affective domain.
A study by Chen et al. (2022) found similar results, where cognitive and psychomotor domain scores were more significantly improved than in the affective domain.
It is possible that due to this domain relating to students' emotions and feelings, there is naturally more variation in how an individual may emotionally relate to their learning, given their inherent traits and prior experiences (Kang et al.2019).
We established through Theme 4 that while some students could have had positive emotional responses such as enjoyment or satisfaction, others felt frustration or stress, as the VPs contain elements of needing to navigate new and challenging clinical scenarios.
It is possible that the usability issues of device compatibility (or lack thereof) and software glitches manifested in a negative reaction affecting the affective domain scores.
Taken all together, these findings demonstrate that pediatric VPs implemented into the Audiology curriculum to supplement other learning activities are able to drive student confidence and add to the evidence base for digital simulation use in the profession.
In Theme 5, we established that students desire VPs to remain in the curriculum and wish for more complex and non‐technical skills focused simulations that could afford them practice with higher order reasoning and communication‐based tasks.
Implementation of communication‐focused simulations into Audiology training appears to be an area gaining more attention yet requires more research to establish how effective this may be for learning (Kanji et al.2024).
With advances in artificial intelligence technology, it is likely that highly realistic VPs focused on communication skill development will be integrated into Audiology training programs in the future, as has been seen in other disciplines (Bowers et al.2024).
It would be of interest to compare the pediatric VPs with other learning activities focused on acquisition of behavioral hearing test skills for pediatric clients, given Chang et al. (2022) found increases in self‐efficacy following use of their nursing digital simulations when compared to lectures.
Evidence in Audiology does suggest that similar knowledge levels can be achieved when comparing digital simulations to a traditional problem‐based learning (PBL) activity (Tomlin et al.2023), however a comparison in self‐efficacy was not established in this study.
Our post hoc power analysis suggested adequate power to detect a medium effect size, so future interventions should carefully consider usability and target emotional aspects of learning to address the lack of improvement in affective domain self‐efficacy and achieve a more substantial effect.
As this study was voluntary for students to participate in, a degree of self‐selection bias may be present, as students who were willing to complete the surveys may be more motivated learners than those who did not.
Furthermore, as the VPs remained available to students following their official use within workshops, this could mean certain students that were motivated to revisit them multiple times may have achieved different gains in self‐efficacy than those only completing them once.
Finally, given the nature of this study taking place over a semester of learning, there is the potential for confounding variables contributing to results (in the form of concurrent coursework or clinical placements).
It is possible that these other experiences may contribute to performance gains and increased confidence when using VPs.
This study indicates that pediatric behavioral hearing assessment VPs can drive increased self‐efficacy in performing clinical skills.
By implementing carefully designed VPs that are user‐friendly, use educational frameworks, and are clearly scaffolded within the Audiology curriculum, we saw changes in cognitive and psychomotor self‐efficacy.
Our findings also suggest that affective domain self‐efficacy may be affected by the user experience, which was affected by device compatibility and software glitches.
Our thematic analysis further revealed that students valued the VPs in affording them an opportunity to gain experience outside of a clinical environment or lecture and that feedback provided was key to their learning.
Students also reported how they best see pediatric VPs fitting into the curriculum and expressed a desire for more complex and communication‐based VPs going forward.
In summary, these findings add to a growing evidence base on the educational value of VPs within Audiology and the wider health care field.
This study suggests that well‐designed VP simulations can enhance audiology students' self‐efficacy in performing pediatric behavioral hearing assessments, in the cognitive and psychomotor domains, and should be considered as a valuable supplementary learning tool in audiology curricula.
By allowing students to practice skills and receive formative feedback in a risk‐free environment prior to clinical placements, these simulations may better prepare them for real‐world pediatric assessments and facilitate transfer of learning to clinical practice.